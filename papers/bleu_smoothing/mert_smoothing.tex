%
% File acl08.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt]{article}
\usepackage{acl08}
\usepackage{times}
\usepackage{latexsym}
\usepackage{mlapa}
\usepackage{amsmath, amsthm}
\usepackage{amssymb}
\usepackage{epsf}
\usepackage{graphicx}    % needed for including graphics e.g. EPS, PS
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algorithmic}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\eval}{eval}

\abovedisplayskip 2.0pt plus2pt minus2pt%
\belowdisplayskip \abovedisplayskip
\renewcommand{\baselinestretch}{0.99}
\newcommand{\captionfonts}{\footnotesize}


\title{Regularization and Search for Minimum Error Rate Training}

\author{
Daniel Cer\textnormal{,} Daniel Jurafsky\textnormal{, and} Christopher D. Manning\\
Stanford University\\
Stanford, CA 94305\\
{\tt {cerd,jurafsky,manning}@cs.stanford.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell's method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell's method and coordinate descent.
 
%points: (i) MERT is the most popular tool for tuning stat MT models, (ii) we explore search procedures and regularization of the error surface (iii) we show gains for MERT stochastic search +0.98 BLEU on MT03 \& +0.61 on MT05, and (iv) gains for smoothing +0.897 on MT03 \& +0.57 on MT05. 
\end{abstract}

\section{Introduction}


%{\bf BEGINREMOVEME - outline }
%\begin{enumerate}
%  \item State original stat mt formulation \cite{brown1993}, \mbox{$p(e|f) = p(f|e)p(e)$}
%  \item Use of log linear formulation to allow more features \cite{och2002}
%  \item MERT introduced as improvement over log linear \cite{och2003MERT}
%  \begin{enumerate}
%     \item advantages: (i) fits actual error surface, even when
%      such a surface is defined at the corpus level. (ii) finds global maximum along any given search direction (but critical to choose right search directions to find a good point in the large multi-dimensional space.
%     \item disadvantages: (i) exactly fitting the training set may provide inferior generalization results (i.e., no smoothing, margin, etc.) (ii) 
%  \end{enumerate}
%  \item What's explored in this paper
%  \begin{enumerate}
%     \item comparison of search strategies (Powell, coordinate ascent, \& randomized)
%     \item comparison of smoothing strategies
%  \end{enumerate} 
%  \item organization of the paper 
%  \begin{enumerate}
%     \item (review of) Minimum Error Rate Training
%     \item Extensions (random search + smoothing)
%     \item Experiments
%     \item Results
%     \item Conclusion
%  \end{enumerate}
%\end{enumerate}
%{\bf ENDREMOVEME - outline}

%Early efforts in statistical machine translation \cite{brown1993} made use of the noisy channel model. As shown in (\ref{noisychannel}), this framework uses Bayes' rule to model the probability of a translation, \mbox{${\bf e}$}, given foreign sentence, \mbox{${\bf f}$}, as the product of a language model over \mbox{${\bf e}$} and an inverted translation model of foreign sentence \mbox{${\bf f}$} given the translation \mbox{${\bf e}$}.

%\begin{equation}
%p({\bf e} | {\bf f}) \propto p_{tm}({\bf f} | {\bf e}) p_{lm}({\bf e})
%\label{noisychannel}
%\end{equation}

%Such a formulation may initially appear to be complicating matters unnecessarily, as, if the distribution \mbox{$p_{tm}({\bf f}|{\bf e})$} can be modeled, it would be more parsimonious to just directly model the conditional distribution \mbox{$p({\bf e} | {\bf f})$}. However, the given formulation has the advantage that it allows the exploitation of two separate knowledge sources in the final model. That is, the inverted translation model, \mbox{$p_{tm}$}, can be used to score translations for content and reordering phenomenon, while the language model, \mbox{$p_{lm}$}, scores potential translations by some measure of grammaticality.

%\emcite{och2002} demonstrated that translation quality can be improved by introducing additional knowledge sources, such an alternative class based language model and a dictionary co-occurrence count feature. Given that the noisy channel model does not readily accept arbitrary novel knowledge sources, the problem was reformulated in terms of a log-linear model. Not only does such a formulation admit arbitrary knowledge sources as features, but also allows all knowledge sources to be weighted discriminatively.

\emcite{och2003MERT} introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models \cite{och2002}. This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score \cite{kishore2002}. This is accomplished by either directly walking the true error surface provided by an evaluation metric w.r.t. the model weights for a given data set or by using gradient-based techniques on a continuous approximation of such a surface. While the true error surface is piece-wise constant and thus cannot be optimized using gradient techniques, \emcite{och2003MERT} provides an approach that performs such training efficiently as long as the number of model weights remains relatively small. 

In this paper we explore a number of variations on MERT. First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell's method and coordinate ascent. Subsequently, results are presented for two regularization strategies. Both strategies allow coordinate ascent and Powell's method to acheive performance that is competitive with stochastic search. 

In what follows we briefly review minimum error rate training, introduce our stochastic search and regularization strategies, and then present experimental results. 

% The remainder of this paper is organized as follows. We begin with a review of minimum error rate training in section 2, and then proceed to present our smoothing and stochastic search strategies in section 3. Section 4  then outlines our experiments, with results being presented and discussed in section 5. Closing remarks and directions for future research are found in section 6.

%{\bf BEGINREMOVEME - alternative equations }
%\begin{equation}
%p({\bf e} | {\bf f}) = p({\bf f} | {\bf e}) p({\bf e})
%\end{equation}

%\begin{equation}
%log p({\bf e} | {\bf f}) = {\bf w}_{tm} p({\bf f} | {\bf e}) + {\bf w}_{lm} p({\bf e})
%\end{equation}

%\begin{equation}
%p_{{\bf \Lambda}} ({\bf e}, {\bf h} |{\bf f}) = \frac {e^{\sum_i \lambda_i f({\bf e}, {\bf h} ,{\bf f})}} { \sum_{{\bf e^\prime} \in {\cal E}} \sum_{{\bf h^\prime} \in {\cal H}} e^{\sum_i \lambda_i f({\bf e},{\bf h},{\bf f})} }
%\label{loglinearp}
%\end{equation}
%{\bf ENDREMOVE - alternative equations }


%\begin{enumerate}
%  \item difficulties in optimizing error surface
%  \begin{enumerate}
%    \item Not only non-convex, but also piece-wise constant
%    \begin{enumerate}
%      \item Introduce cartoon example and use it to illustrate \ref{mertsurface}
%    \end{enumerate}
%    \item Error metrics, like the popular BLEU \cite{kishore2002},
%          defined over the whole data set
%  \end{enumerate}
%\end{enumerate}

\begin{table*}
\begin{center}
\begin{tabular}{|l|lrrr|}
\hline \bf id & \bf Translation  & \mbox{$\bf \log(P_{TM}(f|e))$}  & \mbox{$\bf \log(P_{LM}(e))$} & {\bf BLEU-2} \\
\hline
\mbox{${\bf e}^1$} & This is it  & -1.2 & -0.1 & 29.64 \\
\mbox{${\bf e}^2$} & This is small house   & -0.2 & -1.2 & 63.59 \\
\mbox{${\bf e}^3$} & This is miniscule building   & -1.6 & -0.9 & 31.79  \\
\mbox{${\bf e}^4$} & This is a small house   & -0.1 & -0.9 & 100.00  \\
\hline
{\bf ref}  & This is a small house & & & \\
\hline
\end{tabular}
\end{center}
\caption{
\label{hscores}
Four hypothetical translations and their corresponding \mbox{$\log$} model scores from a translation model \mbox{$P_{TM}(f|e)$} and a language model \mbox{$P_{TM}(e)$}, along with their {\bf BLEU-2} scores according to given the reference translation. The MERT error surface for these translations is given in figure (\ref{mertsurface}).}
\end{table*}

\section{Minimum Error Rate Training}

Let \mbox{${\bf F}$} be a collection of foreign sentences to be translated, with individual sentences \mbox{${\bf f}_0$}, \mbox{${\bf f}_1$}, \ldots, \mbox{${\bf f}_n$}. For any given \mbox{${\bf f}_i$}, the surface form of an individual candidate translation is given by \mbox{${\bf e}_i$}, with the hidden state associated with the derivation of \mbox{${\bf e}_i$} from \mbox{${\bf f}_i$} given by \mbox{${\bf h}_i$}\footnote{This is typically referred to as the alignment.}. Each \mbox{${\bf e}_i$} is drawn from \mbox{${\cal E}$}, which represents all possible strings our translation system can produce. The \mbox{$({\bf e}_i, {\bf h}_i, {\bf f}_i)$} triples are converted into vectors of \mbox{$m$} feature functions by \mbox{${\Psi }:{\cal E}\times{\cal H}\times{\cal F} \rightarrow \mathbb{R}^m$}. The dot product of the representations produced by \mbox{${\Psi}$} and the weight vector \mbox{${\bf w}$} assigns a score to the triples. The idealized translation process then is to find the highest scoring pair \mbox{$({\bf e}_i, {\bf h}_i)$} for each \mbox{${\bf f}_i$}, or rather \mbox{$({\bf e}_i, {\bf h}_i) = \argmax_{({\bf e} \in {\cal E}, {\bf h} \in {\cal H})} {\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}. 


The aggregate \mbox{$\argmax$} for the entire data set \mbox{${\bf F}$} is given by equation (\ref{argmaxeh})\footnote{Here, translation of the entire data set is treated as a single structured prediction problem using the feature vector  \mbox{$\Psi({\bf E}, {\bf H}, {\bf F}) = \sum_i^n \Psi({\bf e}_i, {\bf h}_i, {\bf f}_i)$}}. This gives us \mbox{${\bf E}_{\bf w}$} which represents the set of translations selected by the model for data set \mbox{${\bf F}$} when parameterized by the weight vector \mbox{${\bf w}$}. For dataset \mbox{${\bf F}$}, let's assume we have some automated measure of translation quality \mbox{$\ell$} that maps the collection of translations \mbox{${\bf E}_{\bf w}$} produced by our system onto some real valued loss, \mbox{$\ell:{\cal E}^n\rightarrow \mathbb{R}$}. For instance, for the experiments that follow, the loss corresponds to 1 minus the BLEU score assigned to \mbox{${\bf E}_{\bf w}$} using a collection of reference translations for the sentences in \mbox{${\bf F}$}. 

\begin{equation}
({\bf E}_{\bf w}, {\bf H}_{\bf w}) = \argmax_{({\bf E} \in {\cal E}^n, {\bf H} \in {\cal H}^n)} {\bf w} \cdot \Psi({\bf E}, {\bf H}, {\bf F})
\label{argmaxeh}
\end{equation}

Using n-best lists produced by a decoder to approximate \mbox{${\cal E}^n$} and \mbox{${\cal H}^n$}, MERT searches for the weight vector \mbox{${\bf w}^*$} that minimizes the loss \mbox{$\ell$}. Letting \mbox{$\tilde{\bf E}_{\bf w}$} denote the result of the \mbox{$\argmax$} w.r.t\@. the n-best approximation of the hypothesis space, this search is expressed by equation (\ref{unsmoothmert}). That is, the objective function being optimized during learning is equivalent to the loss assigned by the automatic measure of translation quality, i.e. \mbox{${\cal O}({\bf w}) = \ell (\tilde {\bf E}_{\bf w})$}.

\begin{equation}
{\bf w}^* = \argmin_{\bf w} \ell (\tilde {\bf E}_{\bf w}) 
\label{unsmoothmert}
\end{equation}

After performing the parameter search, the decoder is then re-run using the weights \mbox{${\bf w}^*$} to produce a new set of n-best lists, which are then concatenated with the prior n-best lists in order to obtain a better approximation of \mbox{${\cal E}^n$} and \mbox{${\cal H}^n$}. The parameter search given in equation (\ref{unsmoothmert}) can then be performed over the improved approximation. This process repeats until either the decoder produces no novel entries for the combined n-best list or the weights change by less than some \mbox{$\epsilon$} across iterations.  

Unlike the objective functions associated with other popular learning algorithms such as the conditional likelihood objective used with log-linear models or the regularized hinge loss used with support vector machines, the objective \mbox{${\cal O}$} is piecewise constant over its entire domain. That is, while small perturbations in the weights, {\bf w}, will change the score assigned by \mbox{${\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$} to each triple, \mbox{$({\bf e}, {\bf h}, {\bf f})$}, such perturbations will generally not change the ranking between the candidate translation/derivation pair selected by the argmax,  \mbox{$({\bf e}^*, {\bf h}^*) = \argmax {\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}, and any given competing pair \mbox{$({\bf e}^\prime, {\bf h}^\prime)$}. However, at certain critical points in weight space, the score assigned to some competing pair \mbox{$({\bf e}^\prime, {\bf h}^\prime)$} will exceed that assigned to the prior winner  \mbox{$({\bf e}^*_{w_{old}}, {\bf h}^*_{w_{old}})$}. At this point, the pair returned by \mbox{$\argmax {\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$} will change. The newly selected \mbox{${\bf e}$}  can in turn change the score assigned by the loss, \mbox{$\ell$}, to the translations produced by the model.


\begin{figure}[h]
\vskip 0.2in
\begin{center}
\setlength{\epsfxsize}{3.0in}
\centerline{\epsfbox{mertsurface.eps}}
\vskip -0.15in
\caption{MERT error surface for the candidate translations given in table \ref{hscores}. The regions are labeled with the candidate translations that dominate for that portion of weight space, \mbox{$\argmax {\bf w} \cdot \Psi({\bf e}, {\bf f})$}, as well as the regions' corresponding objective values, \mbox{$1-\ell(\argmax {\bf w} \cdot \Psi({\bf e}, {\bf f}))$}. 
\label{mertsurface}
}
\end{center}
\vskip -0.2in
\end{figure}


This is illustrated in figure (\ref{mertsurface}), which plots the MERT objective function for a simple model with two parameters, \mbox{$w_{tm}$} \& \mbox{$w_{lm}$}, and for which the space of possible translations, \mbox{${\cal E}$}, consists of the four sentences given in table \ref{hscores}\footnote{For this example, we ignore the latent variables, \mbox{${\bf h}$}, associated with the derivation of each \mbox{${\bf e}$} from the foreign sentence \mbox{${\bf f}$}. If included, such variables would only change the graph in that  multiple different derivations would be possible for each \mbox{${\bf e}^j$}. If present, the graph could then include disjoint regions that all map to the same \mbox{${\bf e}^j$} and thus the same objective value.}. Here, the loss \mbox{$\ell$} is defined as \mbox{$1.0 - $}BLEU-2\mbox{$({\bf e})$}. That is, \mbox{$\ell$} is the difference between a perfect BLEU score and the BLEU score calculated for each translation using unigram and bi-gram counts\footnote{This can be contrasted with the typically used BLEU-4, which would include up to 4-gram counts}, for the reference translation ``This is a small house". 

The surface can be visualized as a collection of plateaus that all meet at the origin and then extend off into infinity. The latter property illustrates that the objective is scale invariant w.r.t. the weight vector \mbox{${\bf w}$}. That is, since any vector \mbox{${\bf w}^\prime = \lambda {\bf w}$} \mbox{$\forall_{\lambda > 0}$} will still result in the same relative rankings of all possible translations according to \mbox{${\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}, such scaling will not change the translation selected by the \mbox{$\argmax$}. At the boundaries between regions, the objective is undefined, as, at these points, 2 or more candidates are assigned an identical score by the model. Thus, it is unclear what should be returned by the \mbox{$\argmax$} for subsequent scoring by \mbox{$\ell$}.

Since the objective is piecewise constant, it cannot be minimized using gradient descent or even the sub-gradient method. Two applicable methods include downhill simplex and Powell's method \cite{press2007}. The former attempts to find a local minimum in an \mbox{$n$} dimensional space by iteratively shrinking or growing an \mbox{$n+1$} vertex simplex\footnote{A simplex can be thought of as a generalization of a triangle to arbitrary dimensional spaces.} based on the objective values of the points that make up the simplex and select nearby points.	In contrast, Powell's method operates by starting with a single point in weight space, and then performing a series of line minimizations until no more progress can be made. In this paper, we focus on line minimization based techniques, such as Powell's method.  

\subsection{Global minimum along a line}

Even without gradient information, numerous methods can be used to find, or approximately find, local minima along a line. However, by exploiting the fact that the underlying model-scores assigned to competing hypotheses,  \mbox{${\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}, vary linearly w.r.t. changes in the weight vector, \mbox{${\bf w}$}, \emcite{och2003MERT} proposed a strategy for finding the global minimum along any given search direction. 

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\setlength{\epsfxsize}{3.0in}
\centerline{\epsfbox{mert_line_search.eps}}
\vskip -0.15in
\caption{Illustration of how the model score assigned to each candidate translation varies during a line search along the coordinate direction \mbox{$w_{lm}$} with a starting point of \mbox{$(w_{tm}, w_{lm}) = (1.0, 0.5)$}. Each line in the plot corresponds to the model score assigned to a candidate translation as the weight \mbox{$w_{lm}$} is varied over the range \mbox{$(-3,3)$}.  The lines are labeled with the translation candidate they correspond to. Each vertical band in the graph represents a region where one of the hypotheses is dominate. The transitions between bands are the result of the dotted intersections between the 1-best lines.
\label{mertlinesearch}
}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|rrr|}
\hline \bf Translation  & \bf m  & \bf b & \bf 1-best  \\
\hline
\mbox{$e_1$}     & -0.1 & -1.25  & (0.86,\mbox{$+\infty$}] \\
\mbox{$e_2$}     & -1.2 & -0.8   & (-0.83,0.88) \\
\mbox{$e_3$}     & -0.9 & -2.05  & n/a \\
\mbox{$e_4$}     & -0.9 & -0.55  & [\mbox{$-\infty$},-0.83] \\
\hline
\end{tabular}
\end{center}
\caption{Slopes, \mbox{$m$}, intercepts, \mbox{$b$}, and 1-best ranges for the 4 translations given in table \ref{hscores} during a line search along the coordinate \mbox{$w_{lm}$}, with a starting point of \mbox{$(w_{tm}, w_{lm}) = (1.0, 0.5)$}. This line search in illustrated in figure(\ref{mertlinesearch}).
\label{hlinesearch}
}
\end{table}

\begin{algorithm}[tb]
\begin{algorithmic}
   \STATE {\bfseries Input:} \mbox{${\cal L}$}, \mbox{${\bf w}$}, \mbox{${\bf d}$}, eval
   \STATE \mbox{${\cal I}$} \mbox{$\Leftarrow$} \mbox{$\{\}$}
   \FOR {\mbox{$l \in {\cal L}$}}
     %\STATE find slop and intercepts along search dir
     \FOR {\mbox{$e \in l$}}
         \STATE m\{e\} \mbox{$\Leftarrow$} e.features \mbox{$\cdot$} \mbox{${\bf d}$}
         \STATE b\{e\} \mbox{$\Leftarrow$} e.features \mbox{$\cdot$} \mbox{${\bf w}$}
     \ENDFOR
     % \STATE find initial best at \mbox{$\infty$}
     \STATE best\mbox{$_n$} \mbox{$\Leftarrow$} \mbox{$\argmax_{e\in l}$} m\{e\}
     \COMMENT {b\{e\} breaks ties}
     % \STATE walk along dir, collecting intercepts for nbestlist
     \LOOP
       \STATE best\mbox{$_{n+1}$} = $\argmin_{e\in l}
                  \max\left(0,\frac{b\{best_n\}-b\{e\}}{m\{e\}-m\{best_n\}}\right )$
       \STATE intercept \mbox{$\Leftarrow \max\left(0,\frac{b\{best_n\}-b\{best_{n+1}\}}{m\{best_{n+1}\}-m\{best_n\}}\right )$}
       \IF {intercept \mbox{$>$} 0}
         \STATE add(\mbox{${\cal I}$}, intercept)
       \ELSE
         \STATE {\bf break}
       \ENDIF
     \ENDLOOP
   \ENDFOR
   \STATE add(\mbox{${\cal I}$}, \mbox{$\max({\cal I})+2\epsilon)$}
   \STATE \mbox{$i_{best} = \argmin_{i \in {\cal I}} \eval({\cal L},{\bf w}+(i-\epsilon) \cdot {\bf d})$}
   \STATE {\bf return} \mbox{${\bf w}+(i_{best}-\epsilon) \cdot {\bf d}$}
\caption{
\emcite{och2003MERT}'s line search method to find the global minimum in the loss, \mbox{${\ell}$}, when starting at the point \mbox{${\bf w}$} and searching along the direction \mbox{${\bf d}$} using the candidate translations given in the collection of n-best lists \mbox{${\cal L}$}.
\label{mertlinesearchalg}
}
\end{algorithmic}
\end{algorithm}

The insight behind the algorithm is as follows. Let's assume we are examining two competing translation/derivation pairs, \mbox{$({\bf e}^1, {\bf h}^1)$} \& \mbox{$({\bf e}^2, {\bf h}^2)$}. Further, let's say the score assigned by the model to \mbox{$({\bf e}^1, {\bf h}^1)$} is greater than  \mbox{$({\bf e}^2, {\bf h}^2)$}, i.e. \mbox{\mbox{${\bf w} \cdot \Psi({\bf e}^1, {\bf h}^1, {\bf f}) > {\bf w} \cdot \Psi({\bf e}^2, {\bf h}^2, {\bf f})$}}. Since the scores of the two vary linearly along any search direction, \mbox{${\bf d}$}, we can find the point at which the model's relative preference for the competing pairs switches as \mbox{$p = \frac{{\bf w} \cdot \Psi({\bf e}^1, {\bf h^1}, {\bf f}) - {\bf w} \cdot \Psi({\bf e}^2, {\bf h^2}, {\bf f})}{{\bf d} \cdot \Psi({\bf e}^2, {\bf h^2}, {\bf f}) - {\bf d} \cdot \Psi({\bf e}^1, {\bf h^1}, {\bf f})}$}. At this particular point, we have the equality \mbox{$(p{\bf d} + {\bf w}) \cdot \Psi({\bf e}^1, {\bf h}, {\bf f}^1) = (p{\bf d} + {\bf w}) \cdot \Psi({\bf e}^2, {\bf h}, {\bf f}^2)$}, or rather the point at which the scores assigned by the model to the candidates intersect along search direction \mbox{${\bf d}$}\footnote{Notice that, this point only exists if the slopes of the candidates' model scores along \mbox{${\bf d}$} are not equivalent, i.e. if \mbox{${\bf d} \cdot \Psi({\bf e}^2, {\bf h^2}, {\bf f}) \neq {\bf d} \cdot \Psi({\bf e}^1, {\bf h^1}, {\bf f})$}.}. Such points correspond to the boundaries between adjacent plateaus in the MERT objective, as prior to the boundary the loss function \mbox{$\ell$} is computed using the translation, \mbox{${\bf e}^1$}, and after the boundary it is computed using \mbox{${\bf e}^2$}. 

To find the global minimum for a search direction, \mbox{${\bf d}$}, all that must be done is to move along \mbox{${\bf d}$}, and for each plateau identify all the points at which the score assigned by the model to the current 1-best translation intersects the score assigned to competing translations. At the closest such intersection, we have a new 1-best translation. Moving to the plateau associated with this new 1-best, we then repeat the search for the nearest subsequent intersection. This continues until we know what the 1-best translations are for all points along \mbox{${\bf d}$}. The global minimum can then be found by examining \mbox{$\ell$} once for each of these.

Let's return briefly to our earlier example given in table \ref{hscores}. Starting at position \mbox{$(w_{tm}, w_{lm}) = (1.0, 0.5)$} and searching along the \mbox{$w_{lm}$} coordinate, i.e. \mbox{$(d_{tm}, d_{lm}) = (0.0, 1.0)$}, table \ref{hlinesearch} gives the linesearch slopes, \mbox{$m = {\bf d} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}, and intercepts, \mbox{$b = {\bf w} \cdot \Psi({\bf e}, {\bf h}, {\bf f})$}, for each of the four candidate translations. Using the procedure just described, we can then find what range of values along \mbox{${\bf d}$} each candidate translation is assigned the highest relative model score.  Figure (\ref{mertlinesearch}) illustrates how the score assigned by the model to each of the translations changes as we move along \mbox{${\bf d}$}. Each of the banded regions corresponds to a plateau in the objective, and each of the top most line intersections represents the transition from one plateau to the next. 

Pseudocode for the line search is given in algorithm (\ref{mertlinesearchalg}). Letting \mbox{$n$} denote the number of foreign sentences, \mbox{${\bf f}$}, in a dataset, and having \mbox{$m$} denote the size of the individual n-best lists, \mbox{$|l|$}, the time complexity of the algorithm is given by \mbox{${\cal O}(n m^2)$}. This is seen in that each time we check for the nearest intersection to the current 1-best for some n-best list \mbox{$l$}, we must calculate its intersection with all other candidate translations that have yet to be selected as the 1-best. And, for each of the \mbox{$n$} n-best lists, this may have to be done up to \mbox{$m-1$} times. 

%\begin{enumerate}
%  \item mention alternative of a grid search (others?)
%  \item describe method given in \cite{och2003MERT}, and 
%        illustrated in \ref{mertlinesearchalg} pseudo code
%  \begin{enumerate}
%    \item find all intersections along a search direction
%    \item test objective between interactions
%    \item \mbox{$O(dnm^2)$} -  let \mbox{$n$} denote the number of foreign sentences \mbox{${\bf f}$} in a data set, \mbox{$d$} denote the number of parameters being searched, and letting \mbox{$m$} denote the size of the n-best lists used for learning.
%    \item line search illustrated in \ref{mertlinesearch}
%      \begin{enumerate} 
%         \item for the surface given in \ref{mertsurface}
%         \item starting position 1.0, 0.5
%         \item include table of graph slopes and intercepts \& 1-best line intercepts
%         \item background shading indicates which hypotheses is dominate
%      \end{enumerate}
%   \end{enumerate}
%\end{enumerate}
    
\subsection{Search Strategies}

In this section, we review two search strategies that, in conjunction with the line search just described, can be used to drive MERT. The first, Powell's method, was advocated by \emcite{och2003MERT} when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)\footnote{Moses uses David Chiang's CMERT package. Within the source file mert.c the function that implements the overall search strategy, optimize\_koehn(), is based on Philipp Koehn's Perl script for MERT optimization that was distributed with Pharaoh. }, is used by the MERT utility packaged with the popular Moses statistical machine translation system \cite{koehn2007}. 

\subsubsection{Powell's Method}

Powell's method \cite{press2007} attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is minimized by looping through the search directions and performing a line minimization for each; (iii) A new search direction is constructed that summarizes the cumulative direction of the progress made during step (ii) (i.e., \mbox{${\bf d}_{new} = {\bf w}_{pre_{ii}} - {\bf w}_{post_{ii}}$}). After a line minimization is performed along \mbox{${\bf d}_{new}$}, it is used to replace one of the existing search directions. (iv) The process repeats until no more progress can be made. For a quadratic  function of \mbox{$n$} variables, this procedure comes with the guarantee that it will reach the minimum within \mbox{$n$} iterations of the outer loop. However, since Powell's method is usually applied to non-quadratic optimization problems, a typical implementation will forego the quadratic convergence guarantees in favor of a heuristic scheme that allows for better navigation of complex surfaces. 

\subsubsection{Koehn's Coordinate Descent}

KCD is a variant of coordinate descent that, at each iteration, moves along the coordinate which allows for the most progress in the objective. In order to determine which coordinate this is, the routine performs a trial line minimization along each. It then updates the weight vector with the one that it found to be most successful. While much less sophisticated that Powell, our results indicate that this method may be marginally more effective at optimizing the MERT objective\footnote{While we are not aware of any previously published results that demonstrate this, it is likely that we were not the first to make this discovery as even though Moses' MERT implementation includes a vestigial implementation of Powell's method, the code is hardwired to call optimize\_koehn rather then the routine for Powell.}.


%\begin{enumerate}
%  \item Powell's algorithm  \cite{press2007} (will update to 2007 ref) % to do update reference to 2007 edition
%     \begin{enumerate}
%        \item original strategy suggested by \cite{och2003MERT}
%        \item Basic form: (i) search along coordinates, (ii) construct novel search direction based on progress made during coordinate search (iii) throw away one of the coordinate search directions and replace it with the combined direction (iv) repeat 
%        \item Convergence guarantees
%        \item Basic implementation has problems with search directions becoming linearly dependant. So, typical implementations include additional heuristics that limit when the combined search direction is used to replace one of the existing search directions or to periodically reset all the search directions back to the coordinates. 
%     \end{enumerate} 
%  \item Coordinate ascent 
%     \begin{enumerate}
%        \item basic method
%        \item koehn's specific implementation in CMERT
%        \item intuitively might seems worse than Powell but (i) our results for how often search directions are replaced with Powell (should we mention experimental results too? "as we will see") (ii) may be more robust than applications of Powell's algorithm. 
%     \end{enumerate}
%\end{enumerate}


\section{Extensions}

\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\setlength{\epsfxsize}{6.25in}
\centerline{\epsfbox{smoothing_graphs.eps}}
\vskip -0.15in
\caption{Regularization during line search - from left the right: 
(i) regularization by taking the maximum loss associated with adjacent plateauts,
(ii) regularization by averaging the loss of adjacent plateaus,   
(iii) no regularization. 
Here, each set of bars represents adacent plateaus along the line being searched, with the height of the bars representing their associated loss. The vertical lines indicate the surgate loss values used for the center region under each of the schemes (i-iii). 
\label{smoothinglinesearch}
}
\end{center}
\vskip -0.2in
\end{figure*}

In this section we present and motivate two novel extensions to MERT. The first is a stochastic alternative to the Powell and KCD search strategies, while the second is an efficient method for regularizing the objective.

\subsection{Random Search Directions}

One significant advantage of Powell's algorithm over coordinate descent is that it can optimize along diagonal search directions in weight space. That is, given a model with a dozen or so features, it can explore gains that are to be had by simultaneously varying two or more of the feature weights. In general, the diagonals that Powell's method constructs allow it to walk objective functions more efficiently than coordinate descent \cite{press2007}. However, given that we have a line search algorithm that will find the global minima along any given search direction, diagonal search may be of even more value. That is, similar to ridge phenomenon that arise in traditional hill climbing search, it is possible that there are points in the objective that are the global minimum along any given coordinate direction, but are not the global minimum along diagonal directions.  

However, one substantial disadvantage for Powell is that the assumptions it uses to build up the diagonal search directions do not hold in the present context. Specifically, the search directions are built up under the assumption that near a minimum the surface looks approximately quadratic and that we are performing local line minimizations within such regions. However, since we are performing global line minimizations, it is possible for the algorithm to jump from the region around one minima to another. If Powell's method has already started to tune its search directions for the prior minima, it will likely be less effective in its efforts to search the new region. To this extent, coordinate descent will be more robust that Powell as it has no assumptions that are violated when such a jump occurs. 

One way of salvaging Powell's algorithm in this context would be to incorporate additional heuristics that detect when the algorithm has jumped from the region around one minima to another. When this occurs, the search directions could be reset to the coordinates of the space. However, we opt for a simpler solution, which like Powell's algorithm performs searches along diagonals in the space, but that like coordinate descent is sufficiently simple that the algorithm will not be confused by sudden jumps between regions. 

Specifically, the search procedure chooses directions at random such that each component is distributed according to a Gaussian\footnote{However, we speculate that similar results could be obtained using a uniform distribution over \mbox{$(-1,1)$}}, \mbox{${\bf d}$} s.t. \mbox{$d_i \sim N(0,1)$}. This allows the procedure to minimize along diagonal search directions, while making essentially no assumptions regarding the characteristics of the objective or the relationship between a series of sequential line minimizations. In the results that follow, we show that, perhaps surprisingly, this simple procedure outperforms both KCD and Powell's method.

%   \begin{enumerate}
%      \item idea: choose line search directions randomly
%       \item advantages: (i) can cut across the diagonal like Powell, (ii) but maybe more robust.
%   \end{enumerate}

\subsection{Regularization}
%\subsection{Smoothing of Object During Search}
%   \begin{enumerate}
%      \item idea: during line search, don't just base the objective value of a given point on the true score returned by the evaluation metric at that point. Rather, smooth the objective by looking at adjacent plateaus.
%      \item two variants 
%        \begin{enumerate}
%          \item min: take the minimum of the values of adjacent plateaus
%          \item avg: take the average value of adjacent plateaus
%        \end{enumerate}
%   \end{enumerate}


One potential drawback of MERT, as it is typically implemented, is that it attempts to find the best possible set of parameters for a training set without making any explicit efforts to find a set of parameters that can be expected to generalize well. For example, let's say that for some objective there is a very deep but narrow minima that is surrounded on all sides by very bad objective values. That is, the BLEU score at the minima might be \mbox{$39.1$} while all surrounding plateaus have a BLEU score that is \mbox{$< 10$}. Intuitively, such a minima would be a very bad solution, as the resulting parameters would likely exhibit very poor generalization to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. 

One candidate for performing such regularization is the continuous approximation of the MERT objective, \mbox{${\cal O} = \mathbb{E}_{p_{\bf w}}({\ell})$}. \emcite{och2003MERT} claimed that this approximation achieved essentially equivalent performance to that obtained when using the true loss, \mbox{${\cal O} = \ell$}. However, \emcite{zens07} found that \mbox{${\cal O} = \mathbb{E}_{p_{\bf w}}({\ell})$} achieved substantially better test set performance than \mbox{${\cal O} = \ell$}, even though it performs slightly worse on the data used to train the parameters. Unfortunately, a straightforward implementation of the continuous approximation requires a loss that can be applied at the sentence level. If the evaluation metric of interest does not have this property (e.g. BLEU), a sentence level surrogate must be developed, with successful learning then being tied to how well the surrogate captures the critical properties of the true loss.  

We propose an alternative regularization scheme that operates over the true piecewise constant objective but that mitigates the problem of spurious local minima by smoothing over adjacent plateaus during the line search. That is, when assessing the desirability of any given plateau, we examine, within a fixed window \mbox{$w$}, adjacent plateaus along the direction being searched and combine their evaluation scores. We explore two combination methods, \mbox{$max$} and \mbox{$average$}. The former, \mbox{$max$}, assigns each plateau an objective value that is equal to the maximum objective value in its surrounding window, while \mbox{$average$} assigns a plateau an objective value that is equal to its window's average. Figure (\ref{smoothinglinesearch}) illustrates both methods for regularizing the plateaus and contrasts them with the case where no regularization is used. Notice that, while both methods discount spurious pits in the objective, \mbox{$average$} still does place some value on isolated deep plateaus, and \mbox{$max$} discounts them completely.

Note that one potential weakness of this scheme is the value assigned by the regularized objective to any given point differs depending on the direction being searched. As such, it has the potential to wreak havoc on methods such as Powell's, which effectively attempt to learn about the curvature of the objective from a sequence of line minimizations. 

%{\bf BEGINREMOVEME - outline }

%\section{Experiments}
%   \begin{enumerate}
%     \item Random vs. Coordinate vs. Powell
%     \item Min Smoothing vs. Avg Smoothing vs. No Smoothing 
%   \end{enumerate}

%\subsection{System}
%   \begin{enumerate}
%      \item Word Segmentation \cite{tseng05}
%      \item Phrase Extraction Phrase extraction \cite{koehn2003}
%      \item LM SRLM \cite{stolcke02srilm}
%      \item Decoder comparison to Moses \cite{koehn2007} 
%      \item Implementation of MERT
%      \begin{enumerate}
%         \item Comparison to CMERT for Powell \& Coordinate 
%      \end{enumerate}
%   \end{enumerate}

%{\bf ENDREMOVEME - outline  }
\section{Experiments}

Three sets of experiments were performed. For the first set, we compare the performance of Powell's method, KCD, and our novel stochastic search strategy. We then evaluate the performance of all three methods when the objective is regularized using the average of adjacent plateaus for window sizes varying from 3 to 7. Finally, we repeat the regularization experiment, but using the maximum objective value from the adjacent plateaus. These experiments were performed using the Chinese English evaluation data provided for NIST MT eval 2002, 2003, and 2005. MT02 was used as a dev set for MERT learning, while MT03 and MT05 were used as our test sets. 

For all experiments, MERT training was performed using n-best lists from the decoder of size 100. Training continued until either decoding produced no novel entries for the combined n-best lists or none of the parameter values changed by more than 1e-5 across subsequent iterations.

\subsection{System}

Experiments were run using a right-to-left beam search decoder that achieves a matching BLEU score to Moses \cite{koehn2007} over a variety of data sets. Moreover, when using the same underlying model, the two decoders only produce translations that differ by one or more words 0.2\% of the time. We made use of a stack size of 50 as it allowed for faster experiments while only performing modestly worse than a stack of 200. The distortion limit was set to 6. And, we retrieved 20 translation options for each unique source phrase.

Our phrase table was built using \mbox{$1,140,693$} sentence pairs sampled from the GALE Y2 training data.  The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter \cite{tseng05}.  Phrases were extracted using the typical approach described in \emcite{koehn2003} of running GIZA++ \cite{och03} in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bi-directional lexical reordering model conditioned on the source and the target phrases \cite{tillmann04} \cite{koehn2007}. A 5-gram language model was created using the SRI language modeling toolkit \cite{stolcke02srilm} and trained using the Gigaword corpus and English sentences from the parallel data. 


% \subsection{Training \& Evaluation}
%  \begin{enumerate}
%     \item All MERT parameters tuned on MT02
%     \item Evaluated on MT03, MT05
%   \end{enumerate}
  


\section{Results}

\begin{table}
\begin{center}
\begin{tabular}{|l|rrr|}
\hline \bf Method  & \bf Dev  & \bf Test & \bf Test \\ 
                   & \bf MT02 & \bf MT03 & \bf MT05 \\ \hline
KCD        & 30.967 & 30.778 & 29.580 \\
Powell     & 30.638 & 30.692 & 29.780  \\
Random     & 31.681 & 31.754 & 30.191 \\
\hline
\end{tabular}
\end{center}
\caption{
\label{searchstrat}
Comparison of the three different parameter search strategies: Powell's method, KCD, and stochastic search.}
\end{table}

As illustrated in table \ref{searchstrat}, Powell's method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell's algorithm did not perform better than KCD on the training data\footnote{This indicates that Powell failed to find a deeper minima in the objective, since recall that the unregularized objective is equivalent to the model's dev set performance.}, and in fact actually performed modestly worse, suggests that Powell's additional search machinery does not provide much benefit for MERT objectives. 

Similarly, the fact that the stochastic search obtains a much higher dev set score than either Powell or KCD indicates that it is doing a better job of optimizing the objective than either of the two alternatives. These gains suggest that stochastic search does make better use of MERT's global minimum line search than the alternative methods. Or, alternatively, it strengthens the claim that the method succeeds at combining one of the critical strengths of Powell's method, diagonal search, with coordinate descent's robustness to the sudden jumps between regions that result from global line minimization. Using an approximate randomization test for statistical significance \cite{fk2005}, and with KCD as a baseline, the gains obtained by stochastic search on MT03 are statistically significant (\mbox{$p = 0.002$}), as are the gains on MT05 (\mbox{$p=0.005$}). 

%   \begin{enumerate}
%$     \item Coordinate vs. Powell vs. Random - Table (\ref{searchstrat})
%$     \item Powell \& Coordinate Equivalent (Powell rarely updates search directions, so is typically walks the space in the same way as Coordinate)
%$     \item Random - Does significantly better on test then Powell(chk) \& Coordinate 
%$   \end{enumerate}
 

\begin{table}
\begin{center}
\begin{tabular}{|l|rrrr|}
\hline \bf Method  & \bf Window & \bf Dev & \bf Test & \bf Test \\
                   &            & \bf MT02 & \bf MT03 & \bf MT05 \\ \hline
Coordinate & none &  30.967 & 30.778 & 29.580 \\
           & 3  & 31.665  & {\bf 31.675}  & {\bf 30.266} \\
           & 5  & 31.317  & 31.229  & {\bf 30.182}  \\
           & 7  & 31.205  & {\bf 31.824}  & 30.149 \\ \hline
Powell     & none & 30.638 & 30.692  & 29.780 \\ 
           & 3  & 31.333  & 31.412  & 29.890 \\
           & 5  & 31.748  & {\bf 31.777} &  {\bf 30.334} \\
           & 7  & 31.249  & {\bf 31.571} &  30.161 \\ \hline
Random     & none & 31.681 & 31.754 & 30.191 \\
           & 3  & 31.548  & 31.778  & 30.263 \\
           & 5  & 31.336  & 31.647  & 30.415  \\
           & 7  & 30.501  & 29.336  & 28.372 \\
\hline
\end{tabular}
\end{center}
\caption{
\label{avgsmoothing}
Comparison of different window sizes when regularizing using the average of adjacent plateaus. 
The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p \mbox{$<$} 0.01, over the respective baselines are in bold face. 
}
\end{table}

Tables \ref{avgsmoothing} and \ref{minsmoothing} indicate that performing regularization by either averaging or taking the maximum of adjacent plateaus during the line search leads to gains for both Powell's method and KCD across all the window sizes. However, no reliable additional gains appear to be had when stochastic search is combined with regularization.

It may seem surprising that the regularization gains for Powell \& KCD are seen not only in the test sets but on the dev set as well. That is, in typical applications, regularization slightly decreases performance on the data used to train the model. However, this trend can in part be accounted for by the fact that during training, MERT is using n-best lists for objective evaluations rather than the more expensive process of running the decoder for each point that needs to be checked. As such, during each iteration of training, the decoding performance of the model actually represents its generalization performance relative to what was learned from the n-best lists created during prior iterations. Moreover, better generalization from the prior n-best lists can also help drive subsequent learning as there will then be more high quality translations on the n-best lists used for future iterations of learning. Additionally, regularization can reduce search errors by reducing the risk of getting stuck in spurious low loss pits that are in otherwise bad regions of the space.

%Avg Smoothing - Table(\ref{avgsmoothing})
%\begin{enumerate}
%   \item Smoothing always results in sizable gains for Koehn, statistically significant 
%     \begin{enumerate}
%         \item Window +/- 1: BLEU 31.675, p = 0.000999
%         \item Window +/- 3: BLEU 31.824, p = 0.000999
%         \item Window +/- 1: BLEU 31.229, p = 0.084915 (marginally significant)
%     \end{enumerate}
%   \item Not so helpful for Random \& not consistently helpful for Powell
%     \begin{enumerate}
%        \item Smoothing along the line search only efficient with consistent search directions 
%     \end{enumerate}
%\end{enumerate}

\begin{table}
\begin{center}
\begin{tabular}{|l|rrrr|}
\hline \bf Method  & \bf Window & \bf Dev & \bf Test & \bf Test \\
                   &            & \bf MT02 & \bf MT03 & \bf MT05 \\ \hline
Coordinate & none &  30.967 & 30.778 & 29.580 \\
           & 3  & 31.536  & {\bf 31.927} & {\bf 30.334} \\
           & 5  & 31.484   & {\bf 31.702}  &  29.687  \\
           & 7  & 31.627 &  31.294 & {\bf 30.199} \\ \hline
Powell     & none & 30.638 & 30.692 & 29.780 \\
           & 3  & 31.428  & 30.944  & 29.598 \\
           & 5  & 31.407  & {\bf 31.596} & 30.090  \\
           & 7  & 30.870  & 30.911 & 29.620  \\ \hline
Random     & none & 31.681 & 31.754 & 30.191 \\
           & 3  & 31.179 & 30.898 & 29.529 \\
           & 5  & 30.903 & 31.666 & 29.963 \\
           & 7  & 31.920 & 31.906 & 30.674 \\
\hline
\end{tabular}
\end{center}
\caption{
\label{minsmoothing}
Comparison of different window sizes for smoothing based upon the maximum of adjacent plateaus. 
The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p \mbox{$<$} 0.01, over the respective baselines are in bold face. 
}
\end{table}


% Min Smoothing Table(\ref{minsmoothing})
% \begin{enumerate}
%  \item Same pattern as before. Helps Coordinate, but does not help Random \& doesn't consistently help Powell 
%  \item Coordinate Search \& Smoothing
%  \begin{enumerate}
%   \item Window +/- 1: BLEU 31.927, p = 0.000999
%   \item Window +/- 2: BLEU 31.702, p = 0.001998  
%   \item Window +/- 3: BLEU 31.294, p = 0.079920 (almost significant)
%  \end{enumerate}
%\end{enumerate}


\section{Conclusions}

We have presented two methods for improving the performance of MERT. The first is a novel stochastic search strategy that appears to make better use of MERT's algorithm for finding the global minimum along any given search direction than either coordinate descent or Powell's method. The second is a simple regularization scheme that leads to performance gains for both coordinate descent and Powell's method. However, no further gains are obtained by combining the stochastic search with regularization of the objective.   

One quirk of the regularization scheme presented here is that the regularization applied to any given point in the objective varies depending upon what direction the point is approached from. We are currently looking at other similar regularization schemes that maintain consistent objective values regardless of the search direction.  

\section{Acknowledgments}

{\small
We would like to extend our thanks to our three anonymous reviewers, particularly for the depth of analysis provided. This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM. 
}

% \begin{enumerate}
%   \item Powell vs. Coordinate - equivalent
%   \item Random vs. Coordinate - Random better (has advantage of diag search, but without doing so based on assumptions about the surface)
%   \item Smoothing along line search - always a win for Coordinate, not reliably a win for methods that vary the search directions which are used (i.e. Random \& Powell)
%   \item future work
%      \begin{enumerate}
%         \item more features
%         \item smoothing not just along a line - should allow wins for Powell \& Random
%      \end{enumerate}
% \end{enumerate}

\bibliography{mert_smoothing}
\bibliographystyle{mlapa}

\end{document}


