%
% File acl08dd.tex
%
% Contact: daniel.cer@gmail.com

\documentclass[11pt]{article}
\usepackage{acl08}
\usepackage{times}
\usepackage{latexsym}
\usepackage{mlapa}
\usepackage{amsmath, amsthm}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\DeclareMathOperator*{\argmax}{argmax}

\title{Maximum Utility Training for Statistical Machine Translation} 

% omitted for blind review
\author{}

\begin{document}
\maketitle
\begin{abstract}
Due to limitations in what can be achieved with in a re-ranking framework, recently there has been increased interest in feature rich decoding. Unfortunately, the popular unsmoothed variant of the minimum error rate training (MERT) algorithm scales poorly as the size of the feature space increases. As such, prior work has made use of alternative margin based algorithms. In this paper, we investigate discriminative training of a feature rich decoder using two related probabilistic algorithms that can be seen as an online adaptation of smoothed MERT. The proposed algorithm requires significantly fewer training iterations than unsmoothed MERT and achieves competitive performance using a standard feature set. Further, with the introduction of a rich feature set, we achieve a X BLEU point gain over the unsmoothed MERT baseline. 
\end{abstract}

\section{Introduction}

Early efforts in statistical machine translation \cite{brown1993} made use of the noisy channel model. As shown in (\ref{noisychannel}), this framework uses Bayes rule to model the probability of a translation, ${\bf e}$, given foreign sentence, ${\bf f}$, as the product of a language model over ${\bf e}$ and an inverted translation model of foreign sentence ${\bf f}$ given the translation ${\bf e}$. 

\begin{equation}
p({\bf e} | {\bf f}) \propto p_{tm}({\bf f} | {\bf e}) p_{lm}({\bf e})
\label{noisychannel}
\end{equation}

Such a formulation may initially appear to be complicating matters unnecessarily, as, if the distribution $p_{tm}({\bf f}|{\bf e})$ can be modeled, it would be more parsimonious to just directly model the conditional distribution $p({\bf e} | {\bf f})$. However, the given formulation has the advantage that it allows the exploitation of two separate knowledge sources in the final model. That is, the inverted translation model, $p_{tm}$, can be used to score translations for content and reordering phenomenon, while the language model, $p_{lm}$, scores potential translations by some measure of grammaticality. 

\emcite{och2002} demonstrated that translation quality can be improved by introducing additional knowledge sources, such an alternative class based language model and a dictionary co-occurrence count feature. Given that the noisy channel model does not readily accept arbitrary novel knowledge sources, the problem was reformulated in terms of a log-linear model. Not only does such a formulation admit arbitrary knowledge sources as features, but also allows all knowledge sources to be weighted discriminatively.

Subsequent work by \emcite{och2002b} introduced two alternative training regimes to the log-likelihood objective traditionally used to train log-linear models. Known as minimum error rate training (MERT), both approaches attempt to improve translation quality by fitting an automatic translation evaluation metric, such as translation BLEU score \cite{kishore2002}. One approach, unsmoothed MERT, does this by directly walking the error surface provided by an evaluation metric w.r.t. the model weights. While this surface is piece-wise constant and thus cannot be optimized using gradient based techniques, \emcite{och2002b} provide an algorithm that performs such training efficiently as long as the number of model weights remains relatively small. The other approach, smoothed MERT, makes use of a smooth approximation of the error surface, realized as the expected loss given the model, $\sum_{{\bf f} \in {\cal D}} E_{p({\bf e}|{\bf f}, {\bf \Lambda})} Error({\bf e})$. While \emcite{och2002b} found both smooth and unsmoothed MERT performed comparably well, only unsmoothed MERT has been widely adopted for parameter optimization within the machine translation community.

Due to problems in scaling unsmoothed MERT to large numbers of parameters, recent work involving machine translation systems that include large numbers of decoding features have abandon MERT training in favor of large margin methods \cite{liang2006} \cite{tillmann2006} \cite{watanabe2007} \cite{arun2007}. Given that smoothed MERT, unlike unsmoothed MERT, scales well to arbitrary feature sets, there is a significant gap in the literature regarding its application to feature rich decoding. Not only is smoothed MERT a logical baseline to which other learning algorithms can be benchmarked against, the fact that the resulting model has a straightforward probabilistic interpretation can be beneficial in some domains (e.g., for a task, such as cross language information extraction, where the resulting MT system is incorporated as a component in a larger pipeline of language processing tools).  

In this paper, we present results for two online variants of smooth MERT both on a set of modern baseline features and in the context of feature rich decoding with millions of binary features. The remainder of this paper is outlined as follows: section 2 reviews log-linear models and their application to machine translation, section 3 introduces two variants of smooth MERT and discusses their relative tradeoffs, section 4 presents experimental results, and section 4 has closing remarks. 



\section{Log Linear Models for Statistical Machine Translation}

As first introduced by \emcite{och2003}, log linear models allow for the use of arbitrary features within a statistical machine translation system. As illustrated by (\ref{loglinearp}), given foreign sentence ${\bf f}$, they directly model the posterior of a translation, ${\bf e}$, and a hidden state, ${\bf h}$, which represents the manner in which ${\bf e}$ is derived. The feature functions, $f_i$, inspected $({\bf e}, {\bf h} ,{\bf f})$ triples. While, in principle, such feature functions can be of the character of the binary feature functions widely utilized for other natural language processing tasks, in machine translation they typically provide basic summary statistics (e.g. the length of ${\bf e}$),   or real valued scores from non-trivial knowledge sources (e.g. the $\log p({\bf e})$ according to an n-gram language model, or the $\log$ probability assigned by a generative translation model to $p({\bf f}|{\bf e})$ or $p({\bf e}|{\bf f})$). 


\begin{equation}
p_{{\bf \Lambda}} ({\bf e}, {\bf h} |{\bf f}) = \frac {e^{\sum_i \lambda_i f({\bf e}, {\bf h} ,{\bf f})}} { \sum_{{\bf e^\prime} \in {\cal E}} \sum_{{\bf h^\prime} \in {\cal H}} e^{\sum_i \lambda_i f({\bf e},{\bf h},{\bf f})} }
\label{loglinearp}
\end{equation}

To compute the true predicted probability of a sentence ${\bf e}$ given ${\bf f}$, and thus to compute the true most likely translation, it is necessary to sum out the hidden variables. As shown in \ref{sumout}, this involves summing over a potentially vast set of hidden states ${\cal H}$. Since computing such a sum would generally be intractable, machine translation decoders produce translations by searching for the highest scoring translation hidden state pair, i.e. $\argmax_{{\bf e}, {\bf h}} p({\bf e} {\bf h} | {\bf f})$. Note that the normalization term in (\ref{loglinearp}), $Z({\bf f}) = \sum_{{\bf e^\prime} \in {\cal E}} \sum_{{\bf h^\prime} \in {\cal H}} e^{\sum_i \lambda_i f({\bf e},{\bf h},{\bf f})}$, is constant for any given ${\bf f}$ and that $e^x$ is monotone in its argument. Consequently, $\argmax_{{\bf e}, {\bf h}} p({\bf e}, {\bf h} | {\bf f})$ is particularly convenient as it can be computed over just the weighted sum of features, as in $\argmax_{{\bf e}, {\bf h}}\sum_i \lambda_i f({\bf e}, {\bf h} ,{\bf f})$.

\begin{equation}
p({\bf e}|{\bf f}) = \sum_{{\bf h} \in {\cal H}}^{}p({\bf e}, {\bf h} | {\bf f})
\label{sumout}
\end{equation}

In most domains, and initially for machine translation {\cite och2003}, log linear models are parameterized using a conditional log likelihood objective. As shown, in (\ref{loglike}), for machine translation, this objective attempts to maximize the conditional probability of a set of target translations, $e^\ast$, given their corresponding foreign sentence $f$ over training set ${\cal D}$. Such target translations can correspond to human generated reference translations, or translations produced by the decoder that most closely match the reference translations according to some error metric.  
 
\begin{equation}
{\cal L}({\cal D} : {\bf \Lambda}) = \sum_{({\bf e^{\ast}}, {\bf f}) \in {\cal D}}^{} \log p_{\bf \Lambda} ({\bf e^{\ast}} | {\bf f}) 
\label{loglike}
\end{equation}

As given in (\ref{loklikesolution}) and due to the existence of hidden state, at solution, this objective parameterizes the model so that the expected feature counts given both ${\bf e}$ and ${\bf f}$ are harmonized with the expected feature counts given just ${\bf f}$ \cite{koo2005}. This differs from more typical log-linear models without hidden state, in which the expected feature counts are trained to match those actually observed in the training data.  

\begin{equation}
E_{p({\bf h}|{\bf e}, {\bf f})} f_i({\bf e},{\bf h},{\bf f}) = E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f})
\label{loglikesolution}
\end{equation}

Finally, unlike the previously discussed $\argmax{{\bf e}, {\bf h}} p({\bf e}, {\bf h} | {\bf f})$, optimizing the conditional likelihood objective does require computing a value for the normalization term $Z({\bf f}) = \sum_{{\bf e^\prime} \in {\cal E}} \sum_{{\bf h^\prime} \in {\cal H}} e^{\sum_i \lambda_i f({\bf e},{\bf h},{\bf f})}$. Note that this term involves summing over the set of all possible translations ${\cal E}$ and the set of possible hidden states ${\cal H}$. Since both such sums are generally intractable, in practice $Z({\bf f})$ is approximated using large n-best lists of translation hidden state pairs, ({\bf e},{\bf h}).

% log linear models for mt
% how reference is chosen - > minimum word error rate with reference
% how Z is computed

\section{Fitting an Evaluation Metric}

Simply optimizing the conditional probability of selected target translations risks missing subtleties in the evaluation metric that will be ultimately used to judge the quality of the resulting translation system. For instance, certain variation from the selected targets may still result in a relatively high evaluation score. However, other variations might be severely punished. One example of this is the severe degree to which the BLEU evaluation metric \cite{kishore2002} punishes translations that it judges to be too short while being more lenient with translations that include some additional material. 

As illustrated by \emcite{och2002b}, it is possible to significantly boost the performance of a system w.r.t. an evaluation metric by exploiting an objective that incorporates the selected metric. The proposed techniques, presented as minimum error rate training (MERT), are able to improve the true quality of the translation system at least to the extent that the evaluation metric correlates with human judgments. 

\subsection{Unsmoothed MERT}
While \emcite{och2002b} presented two approaches to fitting an evaluation metric, the one that has been widely adopted, unsmoothed MERT, involves simply making the objective function equivalent with the evaluation metric. As show in (\ref{unsmoothmert})\footnote{As a notational convenience, we assume that the score assigned by the selected evaluation metric, $\ell$, to the collection of translations for some data set ${\cal D}$ can be decomposed into a sum of evaluation scores assigned to individual translations. Notably, this is not possible for the popular BLEU evaluation metric.}, for any given foreign sentence ${\bf f}$, this objective function only examines the 1-best translation selected by the model, denoted here as $\argmax_{{\bf e}, {\bf h}} p({\bf e}, {\bf h} | {\bf f})$. Note that, the selected 1-best translation will generally not be changed for small perturbations in the model weights. While weight perturbations will change the probabilities assigned to the individual translations, small changes will tend not to change the relative rank of the 1-best translation with say the second best. However, at certain critical points in the weight space, a new 1-best will be selected. 

\begin{equation}
{\cal M}_{unsmooth} = \sum_{{\cal D}} \ell(\argmax_{{\bf e}, {\bf h}} p({\bf e}, {\bf h} | {\bf f}), {\bf f})
\label{unsmoothmert}
\end{equation}

Such an objective is said to be piece-wise constant, as it is characterized by a collection of discontinuous large flat plateaus. While this makes the error surface not amendable to standard gradient based methods, \emcite{och2002b} presented a technique that finds the global minima along any given search direction. \emcite{och2002b} combines this technique with Powell's method \cite{press1992} in order to find a local minimum for the objective function. 

While unsmoothed MERT has the advantage that it sets model weights to exactly optimize the selected evaluation metric, it also has the significant drawback that, as compare to other parameterization methods, each additional model weight being optimized brings with it a relatively high cost. Specifically, letting $n$ denote the number of foreign sentences ${\bf f}$ in a data set, $d$ denote the number of parameters being searched, and letting $m$ denote the size of the n-best lists used for learning, the optimization procedure described in \emcite{och2002b} has a time complexity of $O(dnm^2)$ \footnote{\emcite{och2002b}'s algorithm operates by identifying the interceptions of the score assigned to the current 1-best and the other candidates on the n-best list along a given search direction. Finding each intercept requires looping over all of the candidates that have a positive slop and have yet to be the 1-best. Further, since there maybe up to $m-1$ intercepts for any given n-best list, identifying the intercepts is order $O(m^2)$. The line search requires the intercepts for all $n$ n-best lists, and Powell's algorithm will perform $d$ line searches per search iteration.} per search iteration.  As such, while it can be easily used to optimize tens of features, it would not be a good choice for models containing say tens of millions of features. This limitation is not problematic for typical machine translation models which only include anywhere from 8 to a couple dozen features. 

\subsection{Smoothed MERT}

The second approach presented by \emcite{och2002b} makes use of a smooth approximation of the selected evaluation metric. This approximation is obtained by defining the objective not just in terms of the evaluation score assigned to the 1-best translation, but rather in terms of a probability weighted combination of the evaluation scores assigned to all possible translations. As shown in (\ref{smoothmert}), such an objective represents the expected value of the evaluation metric, as predicted by the model. 

\begin{equation}
\begin{split}
{\cal M}_{smooth}({\cal D} : {\bf \Lambda}) = \sum_{{\bf f} \in {\cal D}}^{}\ \sum_{{\bf e} \in {\cal E}} \sum_{{\bf h} \in {\cal H}} p_{\bf \Lambda} ({\bf e},  {\bf h} | {\bf f}) \ell({\bf e}, {\bf f}) \\
{\cal M}_{smooth}({\cal D} : {\bf \Lambda}) = E_{p({\bf e},{\bf h} | {\bf f})} \ell({\bf e}, {\bf f})
\end{split}
\label{smoothmert}
\end{equation}

Since $p_{\Lambda}({\bf e}, {\bf h} | {\bf f})$ is represented using a log linear model, the ${\cal M}_{smooth}$ objective has smooth first and second order derivatives and thus is amenable to optimization using a variety of gradient based techniques. Given that gradient methods incur a relatively small incremental cost for each parameter that needs to be optimized, this smoothed MERT objective, unlike unsmoothed, can be used to optimize models that include large rich features sets (e.g., with potentially millions of distinct binary features). However, this variant of MERT has been neglected by recent work in feature rich decoding in favor of margin based methods \cite{liang2006} \cite{tillmann2006} \cite{watanabe2007} \cite{arun2007}.

\subsection{Loss vs. Utility}

As just presented, smoothed MERT defines its objective in terms of how much the evaluation metric penalizes various erroneous translations. Accordingly, terms in the objective, $\sum_{\bf f} \sum_{\bf e} \sum_{\bf h} p({\bf e}, {\bf h} | {\bf f})$, associated with high loss $\ell$ contribute significantly to the objective while those associated with very low loss contribute more negligibly. In the extreme case, terms associated with prefect translations, i.e. $\ell({\bf e}, {\bf f})=0$, are essentially dropped from the objective. Consequently, such zero loss translations then only contribute to the objective via their presence in the normalization term $Z({\bf f}) = \sum_{{\bf e^\prime} \in {\cal E}} \sum_{{\bf h^\prime} \in {\cal H}} e^{\sum_i \lambda_i f({\bf e},{\bf h},{\bf f})}$.  

Moreover, optimizing this objective using gradient based methods will put significant direct pressure on the model weights to push them away from translations that the evaluation metric judges to be very bad. However, only diffuse indirect pressure on the model weights will be applied toward the selection of translations with high evaluation scores. 

As such, we propose an alternative formulation, {\bf maximum utility training} (MUT), based on translation utility, $u$, rather than loss, $ell$. Assuming loss $\ell \in [0,1]$, $u$ can be defined for our purposes as $u({\bf e},{\bf f}) = 1.0 - \ell({\bf e}, {\bf f})$. As such, perfect translations, i.e. those assigned a loss of 0, are assigned a utility of 1. Reformulating the objective in terms of utility renders \ref{utilityobj}. In contrast to the MERT objective, the MUT objective drops terms with maximal loss and puts significant weight on terms with low loss.

% utility
\begin{equation}
\begin{split}
{\cal U}({\cal D} : {\bf \Lambda}) = \sum_{{\bf f} \in {\cal D}}^{}\ \sum_{{\bf e} \in {\cal E}} \sum_{{\bf h} \in {\cal H}} p_{\bf \Lambda} ({\bf e},  {\bf h} | {\bf f}) u({\bf e}, {\bf f}) \\
{\cal U}({\cal D} : {\bf \Lambda}) = E_{p({\bf e},{\bf h} | {\bf f})} u({\bf e}, {\bf f})
\end{split}
\label{utilityobj}
\end{equation}

As show in (\ref{l2u}) \& (\ref{l2m}), l2 regularization can be introduced to both the MERT \& MUT objectives in order to avoid over fitting. Doing so is comparable to using spherical Gaussian priors when training a log-linear model according to a conditional likelihood objective. 

% l2 regularization
\begin{equation}
\begin{split}
{\cal O}_{\cal U}&({\cal D} : {\bf \Lambda}) = \\&\sum_{{\bf f} \in {\cal D}}^{}\ \sum_{{\bf e} \in {\cal E}} \sum_{{\bf h} \in {\cal H}} p_{\bf \Lambda} ({\bf e},  {\bf h} | {\bf f}) u({\bf e}, {\bf f}) - \frac{\alpha}{2} ||{\bf \Lambda}||_2^2
\end{split}
\label{l2u}
\end{equation}

% expected loss 
\begin{equation}
\begin{split}
{\cal O}_{\cal M}&({\cal D} : {\bf \Lambda}) = \\&\sum_{{\bf f} \in {\cal D}}^{}\ \sum_{{\bf e} \in {\cal E}} \sum_{{\bf h} \in {\cal H}} p_{\bf \Lambda} ({\bf e},  {\bf h} | {\bf f}) \ell({\bf e}, {\bf f}) + \frac{\alpha}{2} ||{\bf \Lambda}||_2^2
\end{split}
\label{l2m}
\end{equation}

For a single data point $\{{\bf f}\}$, (\ref{partou}) \& (\ref{partom}) give partial derivates of both objectives w.r.t. feature weight $\lambda_i$. Since all terms in the partial of ${\cal O}_{\cal U}$ are multiplied by both $u({\bf e}, {\bf f})$ and $p({\bf e}, {\bf h} | {\bf f})$, the gradient will be influence the most by translations that have both a high evaluation score as well as a high probability according to the model. While this leads to a scenario where by metaphorically 'the rich get richer', this is to some extent desirable as some translations that achieve a high evaluation score maybe generated using a pathological hidden state (e.g., an unusually high amount of word scrambling may be necessary to generate some high scoring translation ${\bf e}$ from the given foreign sentence ${\bf f}$). Thus, it will typically be beneficial to dampen the influence of a translation that looks highly improbable given the current model weights even though it achieves a high score w.r.t. the evaluation metric. In contrast to ${\cal O}_{\cal M}$, since ${\cal O}_{\cal U}$ multiplies all terms by $\ell({\bf e}, {\bf f})$ and $p({\bf e}, {\bf h} | {\bf f})$, it can be seen as attending the most to translations that the model thinks are highly probable but that achieve a low evaluation score. As such, it attempts to downgrade translations that are currently overrated according to the model, with doing so necessarily increasing the probabilities assigned to other potentially underrated translations. 
% derivative
\begin{equation}
\begin{split}
\frac {\partial {\cal O}_{\cal U} (\{{\bf f}\})}{\partial \lambda_i} = \sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} u({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) ( f_i({\bf e},{\bf h},{\bf f})\\ - E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) ) - \alpha \lambda_i
\end{split}
\label{partou}
\end{equation}

\begin{equation}
\begin{split}
\frac {\partial {\cal O}_{\cal M} (\{{\bf f}\})}{\partial \lambda_i} = \sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} \ell({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) ( f_i({\bf e},{\bf h},{\bf f})\\ - E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) ) + \alpha \lambda_i
\end{split}
\label{partom}
\end{equation}

As illustrated by (\ref{usol}), modulo the regularization term $\alpha\lambda_i$, at solution, $\partial {\cal O} / \partial \lambda_i = 0$, the MUT objective attempts to harmonize the expected value of the utility weighted feature function $f_i$ with its unweighted expected value multiplied by the average utility of all the translations, $E_{p({\bf e},{\bf h}|{\bf f})} u({\bf e}, {\bf f})$. Similarly, as shown in (\ref{msol}), the MERT objective attempts to harmonize the loss weighted expected value of the feature function with the unweighted expected value of the feature function times the average loss over all translations. 

% at solution

%\begin{equation}
%\begin{split}
%\sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} u({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) f_i({\bf e},{\bf h},{\bf f}) - \alpha \lambda_i=\\\sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} u({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) 
%\end{split}
%\end{equation}


\begin{equation}
\begin{split}
E_{p({\bf e},{\bf h}|{\bf f})} (u({\bf e}, {\bf f}) f_i({\bf e},{\bf h},{\bf f})) - \alpha \lambda_i=\\ E_{p({\bf e},{\bf h}|{\bf f})} u({\bf e}, {\bf f}) E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) 
\end{split}
\label{usol}
\end{equation}

\begin{equation}
\begin{split}
E_{p({\bf e},{\bf h}|{\bf f})} (\ell({\bf e}, {\bf f}) f_i({\bf e},{\bf h},{\bf f})) + \alpha \lambda_i=\\ E_{p({\bf e},{\bf h}|{\bf f})} \ell({\bf e}, {\bf f}) E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) 
\end{split}
\label{msol}
\end{equation}


% expected loss @ solution
%\begin{equation}
%\begin{split}
%\sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} \ell({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) f_i({\bf e},{\bf h},{\bf f}) + \alpha \lambda_i=\\\sum_{{\bf e} \in {\cal E}}^{} \sum_{{\bf h} \in {\cal H}} \ell({\bf e}, {\bf f}) p({\bf e},{\bf h} |{\bf f}) E_{p({\bf e},{\bf h}|{\bf f})} f_i({\bf e},{\bf h},{\bf f}) 
%\end{split}
%\end{equation}


\subsection{Optimization}

As previously discussed, given that both the MERT and MUT objective functions have smooth first and second order derivatives, they can be optimized using a variety of gradient based methods. Of these, we believe that stochastic gradient descent (SGD) is a particularly good choice in the context of machine translation for the following reasons: (i) ease of implementation (ii) efficiency at converging to reasonably good solutions in the context of other log-linear models \cite{vishwanathan2006} (iii) robustness to shallow local minima. 


\section{Experiments}

We evaluate the performance of smooth MERT and MUT using both a standard set of baseline features and in the context of richer feature sets that include millions of additional binary features. All experiments are performed over the Chinese English evaluation data provided for NIST MT02, MT03, and MT05. In this regard, MT02 is used as a dev set for automatic tuning of model parameters\footnote{i.e. the data set that would traditionally be called the training set in most other domains.}, while MT03 is used as what we call the 'dev-test' set. That is, while building our system we repeatedly tuned feature weights on MT02 and then evaluated on MT03 in order to roughly gauge generalization performance. Finally, MT05 was held out as a true test set.

% segmentor released 11/17/2007
% from segmentation paper: The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller $n$-grams drawn from all segmentation of the test data. Since the MT training data is subsample with character $n$-grams, it is not biased towards any particular word segmentation. The MT training data contains $1,140,693$ sentence pairs; on the Chinese side there are $60,573,223$ non-whitespace characters, and the English sentences have $40,629,997$ words.

All Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmentor\cite{tseng2005}. $1,140,693$ sentence pairs sampled from the GALE Y2 training data were used for the phrase table, which was prepared using the typical approach described in \cite{koehn2003} of running GIZA++ \cite{och03} in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bi-directional msd lexical reordering model conditioned on both the source and the target phrases\cite{koehn2007}. A 5-gram language model was created using the SRI language modeling toolkit \cite{stolcke02srilm} with the Gigaword corpus and English sentences from the parallel data.
 
For evaluation, we made use of the standard BLEU metric\cite{kishore2002}, which judges translation quality by the geometric mean of n-gram precisions w.r.t. a set of reference translations. However, the standard variant of the metric is not appropriate for individual sentences as the geometric mean will assign a score of zero to all translations that fail to match at least one higher order n-gram. As such, for learning purposes, we utilized a sentence level BLEU score where by counts for higher order n-gram matches are interpolated with counts from lower order n-gram matches such that $cnt^{effective}_n = cnt^{true}_n + 0.1 cnt^{effective}_n$. 

%\begin{equation}
%BLEU(e) = e^{1-r/\min(r,c)} \exp { \sum_{n=1}^{N} w_n \log p_n}
%\end{equation}

Experiments were run using a right to left beam search decoder that achieves a matching BLEU score to the Moses decoder\cite{koehn2007} over a variety of data sets. Moreover, when using the same underlying model, the two decoders only produce translations that differ by one or more words 0.2\% of the time. We made use of a stack size of 50 as it allowed for faster experiments while only performing modestly worse that a stack of 200. The distortion limit was set to 6. And, we retrieved 20 translation options for each unique source phrase.

\subsection{Baseline Features}

For our baseline feature experiments we made use of the following 8 basic features: (i) $\log P_{lm}({\bf e})$ according to the 5-gram language model, (i) $\log p_{tm}({\bf e} | {\bf f})$ \& $\log p_{tm}({\bf f} | {\bf e})$ both according to the translation counts collected during the construction of the phrase table and lexically reweighted by the word translations probabilities, (iii) linear distortion, (iv) phrase penalty (/count), \& (v)  word penalty (/count). We also made use of bi-directional lexical reordering features w.r.t. the monotone, swap, and discontinuous configurations of adjacent source phrases, which introduced 6 additional features. 

Given the modest size of this basic feature set, the resulting model was amendable to optimization using unsmoothed MERT, us-MERT, in addition to smoothed MERT, s-MERT, and MUT. As shown in table (\ref{base-line}), while unsmoothed MERT achieves the best performance, it is followed after a relatively modest gap by MUT which is then followed by s-MERT. This gap can be attributed to the fact that not only are the smoothed objectives approximations of the true underlying evaluation metric, but the smoothed metrics were also trained using the previously described sentence level BLEU metric. However, unlike unsmoothed MERT, the smoothed varieties where able to converge after just 3 passes over the data, with performance reaching respectable levels (+28) after just the first iteration.

% mert base line mt02
%  pwd
% /scr/nlp/data/gale2/acl08dd/experiments/mert_baselines
%  zcat mert-work.grow-diag-final.mt02/run9.out.gz | multi-bleu.perl ../../resources/mt02.refs/ref
% BLEU = 30.46, 79.3/43.2/23.7/13.1 (BP=0.949, ration=0.950)
%
% -bash-3.1$ pwd
%
% max util - mt02
% pwd
% /scr/nlp/data/gale2/acl08dd/experiments/new_h_state_solution
% multi-bleu.perl ../../resources/mt02.refs/ref <  
%    mt02.ebleu_eval_3itrs.ini.mt02.trans
% BLEU = 29.93, 77.5/41.7/22.4/12.1 (BP=0.978, ration=0.978)
%
% min err - mt02
%
% multi-bleu.perl ../../resources/mt02.refs/ref <  mt02.ebleu_eval_err_opt3itrs.ini.mt02.trans
% BLEU = 28.66, 76.7/40.9/21.8/11.7 (BP=0.959, ration=0.960)
%
% pwd
% /scr/nlp/data/gale2/acl08dd/experiments/mert_baselines
% multi-bleu.perl ../../resources/mt03.refs/ref  < mert-gdf.mt02.evalmt03.trans
% BLEU = 30.39, 78.5/43.1/24.2/13.7 (BP=0.934, ration=0.936)
%
% 
% pwd
%
%  multi-bleu.perl ../../resources/mt03.refs/ref  < mt02_err_3iters.ebleu_eval_on_mt03.trans
% BLEU = 28.93, 75.5/40.6/22.4/12.6 (BP=0.948, ration=0.949)
 
\begin{table}
\begin{center}
\begin{tabular}{|l|rrr|}
\hline \bf Method  & \bf Dev & \bf Dev Test & \bf Test \\ \hline
us-MERT     & 30.46 (9)   & 30.39 & X \\
MUT  & 29.93 (3)   & 29.50 & X \\
s-MERT & 28.66 (3)  & 28.93 & X \\
\hline
\end{tabular}
\end{center}

\caption{\label{base-line} Performance using baseline features. Dev, dev test, and test correspond to the MT02, MT03, and MT05 data sets, respectively.}
\end{table}

% ibm model1
%\cite{brown1993}

\subsection{Conclusion}

\bibliography{discrim_decoding}
\bibliographystyle{mlapa}

\end{document}
