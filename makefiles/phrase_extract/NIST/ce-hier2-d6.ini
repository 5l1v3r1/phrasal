# MERT optimized configuration
# decoder /u/mgalley/mt/mt-tools/moses
# BLEU --not-estimated-- on dev /juicy/scr2/nlp2/data/gale/MichelMTsystem/mt-experiments/zh-en.reorder/mt06.prep
# We were before running iteration 9
# finished Fri Mar  7 06:06:34 PST 2008
# decoder /u/nlp/packages/moses-michel/bin/moses-fast
# BLEU 0.354442 on dev /juicy/scr4/nlp4/data/gale2/GrammaticalTrainingSubsetEval/TM/mt06-nw.source.txt
# We were before running iteration 8
# finished Thu Feb 28 04:59:17 PST 2008
### MOSES CONFIG FILE ###
#########################

# input factors
[input-factors]
0

# mapping steps
[mapping]
0 T 0

# translation tables: source-factors, target-factors, number of scores, file 
[ttable-file]
0 0 5 mt06.tables/phrase-table.gz

# language models: type(srilm/irstlm), factors, order, file
[lmodel-file]
0 0 5 mt06.flt_giga.lm.gz

# limit on how many phrase translations e for each phrase f are loaded
# 0 = all elements loaded
[ttable-limit]
20
0

# distortion (reordering) weight
[weight-d]
0.008814

# language model weights
[weight-l]
0.085872

# translation model weights
[weight-t]
0.019243
0.052945
0.042900
0.025499
0.073802

# no generation models, no weight-generation section

# word penalty
[weight-w]
-0.202287

[distortion-limit]
6

# delimiter between factors in input
[factor-delimiter]
|||

[additional-featurizers]
mt.decoder.efeat.HierarchicalReorderingFeaturizer(mt06.tables/lo-hier.msd2-bidirectional-fe.gz,msd2-bidirectional-fe,backtrack,contiuousDefault)

[n-best-list]
nbest
200
