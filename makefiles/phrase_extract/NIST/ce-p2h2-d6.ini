# Automatically generated Phrasal decoder configuration file
# Configuration template: mt06.hier2-d6-v2.ini
# Source Text: mt06.prep
# References: /scr/nlp/data/gale/NIST-MT-eval-data//mt06/chinese/ref
# Created: Mon Jun 30 10:33:28 PDT 2008
# Training Iteration: 0
# Prior n-best Eval: n/a
# Actual Translation BLEU = 33.189, 75.942/43.856/25.770/15.145 (BP=0.983, ration=0.983 47231/48045)
# 
# Precision Details:
# 	0:35868/47231
# 	1:19984/45567
# 	2:11314/43903
# 	3:6397/42239
###########################################################################

# input factors
[input-factors]
0

# mapping steps
[mapping]
0 T 0

# translation tables: source-factors, target-factors, number of scores, file 
[ttable-file]
0 0 5 mt06.tables/phrase-table.gz

# language models: type(srilm/irstlm), factors, order, file
[lmodel-file]
0 0 5 mt06.flt_giga.lm.gz

# limit on how many phrase translations e for each phrase f are loaded
# 0 = all elements loaded
[ttable-limit]
20
0

# distortion (reordering) weight
[weight-d]
0.008814

# language model weights
[weight-l]
0.085872

# translation model weights
[weight-t]
0.019243
0.052945
0.042900
0.025499
0.073802

# no generation models, no weight-generation section

# word penalty
[weight-w]
-0.202287

[distortion-limit]
6

# delimiter between factors in input
[factor-delimiter]
|||

[additional-featurizers]
edu.stanford.nlp.mt.decoder.feat.HierarchicalReorderingFeaturizer(mt06.tables/lo-hier.msd2-bidirectional-fe.gz,msd2-bidirectional-fe)
edu.stanford.nlp.mt.decoder.feat.NGramLanguageModelFeaturizer(mt06.rsc_google.lm.gz,LM2)

[weights-file]
ce-p2h2-d6.wts

[n-best-list]
nbest
200
