## Taken from:
## /scr/nlp/data/gale/GALE-arabic/translate/grow/v3-class/dev07-asr

#########################################
# Size of test-time nbest list:
#########################################

N=200

#########################################
# Java command (must have MT stuff in class
# path; and locales must be set correctly)
#########################################

SYSID=v3-2lm-hr2-v17

#########################################
# Training/test/dev stuff:
#########################################

### MT word-aligned training data:
TRAIN=/scr/nlp/data/gale/GALE-arabic/translate/train-ucb-v3

### Unfiltered LMs:
GIGA=/scr/nlp/data/gale2/ARABIC_LM/IBMclassing/releases/CmtF_giga3_afp_xin.1222.unk.lm.gz
BBN=/scr/nlp/data/gale2/ARABIC_LM/IBMclassing/releases/bbn.1222.unk.lm.gz

### Count-based Google LM:
GOOGLE_CT=/scr/nlp/data/gale2/GoogleNgrams/IBMclassing
GOOGLE_LM=/scr/nlp/data/gale2/ARABIC_LM/IBMclassing/google/google.countlm.3

### Directory containing reference translations:
REFS=/scr/nlp/data/gale/AE-MT-eval-data

### Dev/test corpus and genre identifiers:
DEVID=mt06
TESTID=dev07-asr
GENRE=asr

### References:
DEV_REF=$(REFS)/$(DEVID)
TEST_REF=$(REFS)/$(TESTID)

#########################################
# Language/data specific variables:
#########################################

### Language identifiers:
F=ar
E=en

### How much data to use:
LINES=90000000

#########################################
# Pruning:
#########################################

maxPLen=10

### Minimum p(e|f) probability:
### (Note: filtering based on p(f|e), lex(e|f), lex(f|e)
### isn't really effective)
MINP=1e-3

#########################################
# Phrase extraction heuristics:
#########################################

### Specify alignment merging heuristic:
ALIGN=grow

### Lexicalized re-ordering model identifier:
LO_ID=lo-hier.msd2-bidirectional-fe

### Parameters for lexicalized reordering model:
LO_ARGS=-maxLen 500 -phiFilter $(MINP) -pharaohLexicalizedModel msd2-bidirectional-fe -phrasalLexicalizedModel true  

### Number of columns produced by lexicalized re-ordering extractor:
LO_SZ=8

### Other options:
XOPTS=

#########################################
# Memory and extraction duration:
#########################################

### How much memory for Java and sort:
MEMSIZE=44500m
PMEMSIZE=44500m
PMEMSIZE_BM=44500m

### Whether to compute exact relative frequencies p(f|e). 
### If so, requires two passes over training data. 
### Setting this to false requires only one pass, and 
### generally causes a .2-.3 BLEU point drop.
exactPhiCounts=true

### In how many chunks to split phrase extraction. Setting the
### value to X makes phrase extraction run X times slower,
### though one needs about X times less memory.
### Note: "split" means we split the dev/test-set phrases to score
### into X chunks, and make 1-2 passes over the training data
### for each one of them.
### (if you run out of memory, increase the split value)
SPLIT=-split 1

#########################################
# Pre/post processing:
#########################################

PRE=perl $(SCRIPTS)/remove_unk_before_decode $(maxPLen)
POST=perl $(SCRIPTS)/remove_unk | en_truecaser

#########################################
# Debug flags
#########################################

### Uncomment this to debug Phrasal decoder:
DEBUG_PHRASAL=-DMultiBeamDecoderDebug=true -server -XX:+AggressiveHeap

#########################################
# Phrasal MERT:
# (in case you run a customized version)
#########################################

## generic phrasal mert:
#PM=phrasal-mert.pl
## my personal version:
PM=phrasal-mert-michel.pl

#########################################
# Extension of input files:
# unk : with unk words
# prep : with preprocessing
#########################################

IE=cunk
FE=unk

#########################################
# LM filtering script:
#########################################

LM_FILTER_SCRIPT=filter_lm_unigram

include Makefile.extra
