# Automatically generated Phrasal decoder configuration file
# Configuration template: mt06.v3-2lm-hr2-v17.ini
# Source Text: mt06.cunk
# References: /scr/nlp/data/gale/AE-MT-eval-data/mt06/ref
# Created: Sun Dec  7 16:32:34 PST 2008
# Training Iteration: 2
# Prior n-best Eval: 0.480
# Actual Translation BLEU = 47.849, 80.601/56.137/40.109/28.905 (BP=1.000, ration=1.000 55849/55859)
# 
# Precision Details:
# 	0:45015/55849
# 	1:30343/54052
# 	2:20962/52262
# 	3:14596/50496
###########################################################################

[input-factors]
0

# mapping steps
[mapping]
0 T 0

# translation tables: source-factors, target-factors, number of scores, file 
[ttable-file]
0 0 5 P3-audio2.tables/phrase-table.gz

# no generation models, no generation-file section

# language models: type(srilm/irstlm), factors, order, file
[lmodel-file]
0 0 5 P3-audio2.flt_giga.lm.gz

# limit on how many phrase translations e for each phrase f are loaded
# 0 = all elements loaded
[ttable-limit]
20
0

# distortion (reordering) weight
[weight-d]
0.037171

# language model weights
[weight-l]
0.063071

# translation model weights
[weight-t]
0.039691
0.023897
0.038498
0.045739
0.082644

# no generation models, no weight-generation section

# word penalty
[weight-w]
-0.217795

[distortion-limit]
6

# delimiter between factors in input
[factor-delimiter]
|||

[additional-featurizers]
mt.decoder.efeat.HierarchicalReorderingFeaturizer(P3-audio2.tables/lo-hier.msd2-bidirectional-fe.gz,msd2-bidirectional-fe,backtrack,contiuousDefault)
mt.decoder.feat.NGramLanguageModelFeaturizer(P3-audio2.rsc_google.lm.gz,LM2)
mt.decoder.feat.NGramLanguageModelFeaturizer(P3-audio2.flt_bbn.lm.gz,LM3)

[n-best-list]
P3-audio2.mt06.v3-2lm-hr2-v17.500.200best
200

[weights-file]
GALE-P3-audio.wts
