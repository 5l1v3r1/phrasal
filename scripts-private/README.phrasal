///////////////////////////
Training Research MT Systems with Phrasal
Updated: 15 October 2013
Author: Spence Green

This README explains how to setup an MT system with Phrasal on the Stanford NLP cluster. 

This document assumes that you have a working copy of JavaNLP, and that Phrasal (the Stanford MT package in JavaNLP) is available at $JAVANLP_HOME/projects/mt.

Further, assume that the following two directories have been added to the PATH of your shell:

  $JAVANLP_HOME/projects/mt/scripts
  $JAVANLP_HOME/projects/mt/scripts-private

In the instructions that follow, assume that the first directories above are called $SCR and $SCRPR, respectively.

Finally, the Python scripts in $SCRPR expect Python 2.7. Current, the default Python interpreter on the cluster ('/usr/bin/env python') is Python 2.4. Python 2.7 is here:

  /u/nlp/bin/python2.7
  
##
## Data Cleaning
##
The first step is to prepare you bilingual and monolingual data. We have pre-processors for the following languages: Arabic, Chinese, English, French, and German.

Start with the bilingual data. You should have two sentence-aligned text. Let's call them arabic.txt and english.txt. You need clean and tokenizer them:

  $SCRPR/tokenize.sh English english.txt clean
  $SCRPR/tokenize.sh Arabic arabic.txt clean

These commands will produce the files english.txt.tok.gz and arabic.txt.tok.gz, respectively.

The languages that use deterministic tokenization (e.g., English, French) are very fast. The other processors can be quite a bit slower, so you might want to use the "split" command-line program to partition the data. Then you can write a quick script to distribute the jobs to the cluster.

Now for the monolingual data. If you are using the LDC Gigaword corpora, then you should use this command to extract the data from the gzip'd SGML files in the corpus:

  zcat gigaword-file.gz | $SCRPR/giga_sgml2plain.py > gigaword-file.plain
  $SCRPR/tokenize.sh English gigaword-file.plain clean
  
##
## Word alignment
##
Next, you need to create word alignments. This is easy with the script $SCRPR/align.sh. Before running this script, you should filter the bitext for long sentences:

  $SCRPR/clean-corpus.py -l 100 arabic.txt.tok.gz english.txt.tok.gz

This script will create the files:

  arabic.txt.tok.filt.gz
  english.txt.tok.filt.gz

Now you can run the word aligner script. This script splits the bitext into shards and sends an alignment job to the nlp cluster for each shard. A shard size of 1000000 works well:

  $SCRPR/align.sh arabic.txt.tok.filt.gz ar english.txt.tok.filt.gz en 1000000

Notice that you need to supply the ISO639-1 language codes for the source and target languages.

##
## Language Model
##
Build a language model from the monolingual target data and the target side of the bitext (you might want to filter out data from the same epoch as you dev/test sets). This is easy:

  mk-kenlm.sh


##
## MT Training and Tuning
##
Please see the Phrasal User Guide on the Stanford NLP wiki.