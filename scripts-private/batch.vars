#
# System training parameters from phrasal-train-tune.sh
#

#
# General parameters
#
HOST=`hostname -s`
MEM=30g
JAVA_OPTS="-server -ea -Xmx${MEM} -Xms${MEM} -XX:+UseParallelGC -XX:PermSize=256m -XX:MaxPermSize=256m"

# Threads to use for phrase extraction
THREADS_EXTRACT=4

# Threads to use for batch decoding
THREADS_DECODE=8

EXTRACTOR_OPTS="-DcustomScores=phi_tf,lex_tf,phi_ft,lex_ft,phrasePenalty,count,uniq"
MERT_OPTS="-DsmoothBLEU=true"
DECODER_OPTS="-DMultiBeamDecoderDebug=true -DSRILM=true -Djava.library.path=/scr/nlp/data/gale3/SRILM-JNI/${HOST}"

#
# Resource locations
#
REFDIR=/scr/nlp/data/gale/AE-MT-eval-data
SCRIPTDIR=${JAVANLP_HOME}/projects/mt/scripts-private
CORPUSDIR=/scr/nlp/data/gale3/NIST09/align/merged
CORPUS_SRC=${CORPUSDIR}/good_subset/corpus.ar
CORPUS_TGT=${CORPUSDIR}/good_subset/corpus.en
CORPUS_ALIGN=${CORPUSDIR}/good_subset/corpus.align

#
# Phrase extraction parameters (steps 1 and 5)
#

# Mandatory extraction set format. See Usage of mt.train.PhraseExtract
# for the several different extraction set formats
EXTRACT_SET="-fCorpus $CORPUS_SRC -eCorpus $CORPUS_TGT -align $CORPUS_ALIGN"
THREADS_EXTRACT=4
OTHER_EXTRACT_OPTS="-split 2 -phiFilter 1e-4 -endAtLine 90000000 -maxELen 7"

# Feature extractors
EXTRACTORS=edu.stanford.nlp.mt.train.MosesPharoahFeatureExtractor:edu.stanford.nlp.mt.train.CountFeatureExtractor:edu.stanford.nlp.mt.train.LexicalReorderingFeatureExtractor
EXTRACTOR_OPTS="-DcustomScores=phi_tf,lex_tf,phi_ft,lex_ft,phrasePenalty,count,uniq"

# Lexicalized re-ordering models
LO_ARGS="-hierarchicalOrientationModel true -orientationModelType msd2-bidirectional-fe"


#
# Batch tuning parameters
#
TUNE_MODE=batch
TUNE_SET=mt06.unk
TUNE_SET_NAME=mt06

SEED=`date +%s`
N_STARTING_POINTS=20
NBEST=200
OBJECTIVE=bleu
OPT_FLAGS="-o koehn+cer -s $SEED -F -t $THREADS_DECODE -p $N_STARTING_POINTS"
# For MERT: Use "-o koehn+cer" 
# For PRO: Use "-o pro"

# Decoding parameters
DECODE_SET=mt05.unk
DECODE_SET_NAME=mt05

