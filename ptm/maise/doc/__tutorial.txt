This tutorial takes you through the steps necessary to create, upload, and
retrieve answers for tasks on MTurk.  (We will make use of MTurk's *Sandbox*,
where requesters can test out their tasks and interfaces without incurring any
costs.)  Before proceeding, make sure you've completed the instructions in
Section 5 of the readme file ("Setting up MAISE").

Note: The tutorial is split into "mini sections" separated with --------- .
This does not imply a long wait time or anything like that; just good points to
stop and review what you just did.

Another note: OK, so this tutorial is a little long.  I know that, but I made it
long on purpose, in order to answer your questions before you even think of them.
Trust me, there is nothing too technical here, and most of it is just pointing
out and explaining different little things you will come across, and very little
of it is actually you executing commands or doing any typing.  I estimate that
it will take you 30-60 minutes to go through the entire tutorial, which really
isn't that bad if you think about it!

---------

The tutorial will first give a brief introduction to Amazon's Mechanical Turk.
If you are already familiar with MTurk and its Sandbox mirror site (i.e. you've
posted tasks to MTurk before), you can probably skip this first section.

We will specifically cover the terms: Worker (aka Turker), Requester, HIT (and
HIT batch), "accepting" a HIT, and "submitting" a HIT.  We will also mention the
Sandbox version of MTurk, which allows users like you to create and review what
their tasks would look like, without incurring any actual cost.

As you should know by now, MTurk is a virtual marketplace, where people post
tasks they want completed by human *Workers*.  (Those Workers are also called
"Turkers" in MTurk lingo.)  The person posting a task is called a *Requester*.
If you go to MTurk's webpage, http://www.mturk.com, you will be able to sign in
(using an Amazon.com account) either as a Worker or a Requester.  As an
Amazon.com user, you can use the same Amazon.com account you linked to your
Requester account to log in as a Worker too, so go ahead and do that, by
clicking on the "Worker" link on the top right corner.

Once you're logged in, you will be able to search for the posted tasks using
a keyword search.  For example, if you search for "survey" you will probably get
a good number of results, since MTurk is often used to conduct surveys.

You should note that each of the results has associated with it, among other
things, a title and a reward.  If you click on the title, you will see more
details, such as a description and a list of keywords.  One important detail for
each result is the number of "HITs Available".  A *HIT* is shorthand for Human
Intelligence Task, and refers to a single instantiation of the task at hand.  In
other words, each of the search results you see is a *batch* of HITs, where all
these HITs have the same task details (title, reward, description, etc), but
differ from each other in the actual instantiation.

If a worker is interested in a HIT, and if they are qualified to do it (i.e.
they satisfy the requester's qualification requirements), they may preview the
HIT and then *accept* to work on it.  When a worker "accepts" a HIT, that HIT is
taken off the marketplace and the worker is given a certain time to complete it.
Once they are done, they *submit* the HIT and await the requester's approval for
their answers.  If their answers are approved by the requester, they get
compensated, and their "approved HITs" count goes up by one.  If their answers
are rejected, the worker receives no compensation, and their "rejected HITs"
count goes up by one.  The number of approved vs. rejected HITs is important to
a worker, since a worker's approval rate is usually expected to be high before
they are allowed by requesters to accept HITs, and for this reason workers
typically attempt to perform the task properly in order to avoid getting their
submission rejected and, as a result, their reputation hurt.

Finally, you should know that MTurk also has a *Sandbox* version of its website:

  https://workersandbox.mturk.com/mturk/welcome

Notice that it exactly mirrors the "real" MTurk site.  If you log in, again as
a Worker, you will see an entirely different set of HITs, created by many other
Requesters who are either learning about MTurk or finetuning their tasks.
A Worker may complete these HITs as well, though any "money" they earn is not
real. :)  The Sandbox is an excellent testbed for Requesters, in order to check
if tasks are being displayed properly, without having to pay any actual money to
post the test HITs.  As a requester, you will always have an imaginary $10,000
in your account -- this amount neither decreases nor increases, and the exact
value was chosen arbitrarily by the MTurk developers.

If you're new to MTurk, it is highly recommended that you play around a bit.  Go
ahead and even complete some HITs by other requesters!  When you design and post
HITs to MTurk, it is important to understand what the experience of the worker
is like, and it is helpful to see what other tasks there are out there
"competing" for workers' attention.

---------

To begin, examine the contents of the maisetutorial folder.  Ignore the empty
output folder for now, and take a look at the files in the sources, references,
and systems folders.  You will realize these files correspond to machine
translation submissions for three language pairs: Czech-to-English,
English-to-German, and English-to-French.  Each of the language pairs has
a source file, a reference file, and some system outputs.

Your goal in this tutorial will be to evaluate how close the MT systems' outputs
were to the reference (human-produced) translations.  Instead of using
an automatic metric such as BLEU or TER, we will collect human judgments to
determine which system is best, and how the systems compare to each other.
NAMELY, we will use the "Ranking" evaluation setup, where an annotator is asked
to rank outputs of a few systems based on quality.

To see an example of what this task looks like to the annotator, check out:

  http://bit.ly/ccQDC5

---------

The first thing we need to do is to tell MAISE what it is that we're trying to
do, and where it can find the relevant files.  You do this in a task
specifications file just like the text file maisetutorial/maisetutorial_task-specs.txt .
The format is pretty straightforward, and is meant to be easily readable and
editable by a MAISE user.

The task specification file is provided as input for the first component of
MAISE, called CreateServerInfo.  From the maise/ path, execute the following
command:

  maise> java -cp lib/ CreateServerInfo maisetutorial/maisetutorial_task-specs.txt

(Note: the Java option -cp is the classpath option.  If you prefer, you can add
the maise/lib/ path to your Java classpath environment variable to avoid having
"-cp lib/" in each Java command you execute.)

If you look in the maisetutorial/maisetutorial_output/ folder, you should see
that CreateServerInfo created two things: a "server info" text file, and
a "context HTML" folder that have some subdirectories filled with tiny HTML
files.  We'll get back to those context HTML files later.  For now, examine the
server info file.  You will find that it containts pretty much the same
information in the task specifications file, though in a different format and
in somewhat more detail.

Why did we do this?  Isn't this server info file largely redundant, given the
task specs file?  That is true, but the format of the original task specs is
fairly pleasant for a human to read and edit, which is not the case with the
server info file.  You would probably much rather deal with the format of the
task specs file than deal with the server info file, especially if you are
creating the file from scratch.

THAT SAID, if you want full control over how your tasks are created, you can
certainly edit the server info file.  This first step (using CreateServerInfo)
is only meant to make life easier for you, but in some cases it might make
perfect sense for you to post-edit the server info file manually.  It should be
noted here that the task specs file will not be needed in the later components,
because those components will rely solely on the server info file.

---------

Now, we will create some input files for MTurk: the files that contain actual
instantiations of our tasks, with actual sentences.

This will be the first step that requires you to make some real executive
decisions regarding your tasks.  You will decide how many judgments you will
collect, and who you will allow to give you those judgments.  If you examine the
maisetutorial/maisetutorial_batch-specs.txt file, you will see that we are
interested in creating six batches, each batch dealing with a particular
language pair, and geared toward a particular worker population on MTurk.

(Note: It is very conceivable that you would create many fewer batches than six
in your evaluation, maybe even only one batch.  But when writing the tutorial,
I wanted to showcase as much as possible what it is that you can do here, hence
the many batches.  In any case, you must have at least one batch for each
language pair.)

Again here, it is fairly easy to figure out what the contents of this file mean.
You should notice that, for each batch, you need to specify 7 settings (plus,
optionally, an eighth location setting):

  1) what language pair does this batch involve? (language-pair)
  2) what is the task type for this batch? (task)
  3) how many HITs does this batch include? (batch-size-in-HITs)
  4) how many times do you want each HIT completed? (assignments-per-HIT)
  5) how much do you want to pay (in USD) per assignment? (reward-per-assignment)
  6) what is the minimum approval rate a worker must have to qualify for this batch? (min-approval-rate)
  7) what is the minimum number of previously approved HITs a worker must have to qualify for this batch? (min-approved-HITs)
  8) (OPTIONAL setting) what are the location restrictions a worker must satisfy to qualify for this batch? (location)

Typing seven (or eight) setting values for each batch could get annoying and
repetitive, so you can make use of default values for settings 3-7, as you see
near the top of the file.  Note that you must *explicitly* specify the values
for language-pair and task for *each* batch (and the location, if you want
a location requirement).

For this step, we will use the CreateBatches component.  From the maise/ path,
execute the following command:

  maise> java -Xmx300m -cp lib/ CreateBatches serverInfo=maisetutorial/maisetutorial_output/maisetutorial_server_info.txt batchInfo=maisetutorial/maisetutorial_batch-specs.txt templateLoc=maintaskfiles/

Now take a look at the output folder.  You will find two files for each batch:
an input file and a properties file.  The input file contains data concerning
the actual instantiations of your task, and the properties file contains data
concerning the executive details of the task, such as the reward.  (There is
also a "question" file, which we will talk about in a bit.)

But wait, each of those properties files has a title, a description, and
a number of keywords.  Where did they come from?  Well, if you look in
maise/maintaskfiles/ (which was the value for CreateNewBatches's templateLoc
setting) you will find a properties *template* file.  Now you will realize that
when CreateBatches created a properties file, it first copied the contents of
the template file line by line.  Notice also how it replaced things like
%TOLANG% and %FROMLANG% with the appropriate string for each new properties
file.

Of course, you can edit this template properties file before running
CreateNewBatches.  And of course, you can edit each properties file individually
after it is created.

---------

You might have noticed another file in the output folder, called
maisetutorial-c01.RNK.question .  This is the "question file" for your RNK
tasks.  It tells MTurk where it can find the HTML interface that workers will
use when completing your task.  This HTML interface must be hosted by you,
somewhere publicly accessible.  (The workers will not visit that page directly.
MTurk will show them the page within a frame inside MTurk.  So technically, only
MTurk is acessing the HTML interface.  But since you are not MTurk (are you??),
the interface must be publicly accessible.)

For the ranking task, this HTML interface is also in maise/maintaskfiles/ .  You
will have to modify the return string of the homeURL() function in line 15 to
reflect the location at which you will host the things relevant to this task.
Within that location, you should have a folder called "html", in which you must
place a copy of this HTML file.

You must also copy the maisetutorial_output/context-html/ folder in its entirety
to your publicly accessible html/ folder.

So basically, you need a publicly accessible hosting location, which we will
assume is called:

  http://www.YOUR-HOST.com/PATH/TO/YOUR/HOSTING/LOCATION/ (this is the "base" URL, and should be returned by the homeURL() function)

...and it will have certain publicly accessible content, such as:

  http://www.YOUR-HOST.com/PATH/TO/YOUR/HOSTING/LOCATION/html/RNK.shtml
  http://www.YOUR-HOST.com/PATH/TO/YOUR/HOSTING/LOCATION/html/context-html/plus-minus-0/
  http://www.YOUR-HOST.com/PATH/TO/YOUR/HOSTING/LOCATION/html/context-html/plus-minus-2/

Again, don't forget to edit the homeURL() function in the RNK.shtml file.

One last step to do is to edit the question file to reflect this.  It too will
initially have the generic base URL, which you must replace with the real base
URL.

---------

IMPORTANT: the next step will have you creating HITs and uploading them to
MTurk.  We need to make sure they are being added to the Sandbox instead of the
actual MTurk website.  For this, open the file
maise/MTurkSDKCode/mturk.properties and scroll down to the bottom, to where the
"service_url" variable is defined.  Make sure that the Sandbox definition is the
active one, and that the other definition is commented out.

---------

After CreateNewBatches creates the different files for the different batches, it
also created the file maise/maisetutorial-c01.uploadinfo .  This file contains
all the filenames of the files just created.  As far as the MTurk Java SDK is
concerned, creating a HIT batch requires three files: the input file, the
properties file, and the question file.  We are now at the point where we need
to tell MTurk, for each batch, which files these are.  And that is exactly the
information contained in this file.

And this upload info file will be an argument to the first MAISE component that
actually communicates with MTurk, the "uploader" component.  From the maise/
path, execute the following command:

  maise> ant uploader -Dfile=maisetutorial-c01.uploadinfo

As you might expect, you need an Internet connection for this step to execute
properly.

Once you execute the command, you will see a lot of output telling you about the
HITs being created for the different batches.  As each one is created, it is
made available for the world to see and complete (on the Sandbox version).  As
a requester, you can track your HITs online.  To do so, go to this URL:

  https://requestersandbox.mturk.com/mturk/manageHITs   (or https://requester.mturk.com/mturk/manageHITs when not using the Sandbox)

You will find a (fairly long, 12-page) list of all the individual HITs you just
created, indicating they were successfully uploaded.

Alternatively, if you log in as a _worker_ to the Sandbox (https://workersandbox.mturk.com/),
you can find the HITs by searching for "translation" or "maisetutorial", both of
which were keywords for our HITs.  There may be other results by other
requesters, but there should be six results under your name, corresponding to
the six batches you just created.

---------

At this point, you will be waiting for Turkers to find your task and start
accepting HITs and completing them.  You can retrieve those answers by using
another MAISE component that communicates with MTurk called the "retriever"
component.  From the maise/ path, execute the following command:

  maise> ant retriever -Danswers=maisetutorial-c01.answers.log -Dfield=keywords -Dquery=maisetutorial-c01

What this command does is retrieve the answers for any HIT that has
"maisetutorial-c01" in its keywords field.  (Note that this string matches the
collection-ID we used in the batch specs file.)  It retrieves all the answers
found for any Assignment, and appends those answers to the answer log file (or
if one does not exist, it creates a new one).

The first time you run this, you shouldn't retrieve any answers, because, well,
probably nobody answered any of your HITs yet.  (If you examine the newly
created maisetutorial-c01.answers.log, you will find that it is empty.)

So you should answer one of the HITs yourself!  Go to the workers' section of
MTurk's Sandbox, search for your tasks, and answer a HIT from a batch you are
qualified to do, for instance the English-to-French task.  Click on "View a HIT
in this group" and ACCEPT one of the HITs.  Answer the HIT by ranking the
translations, and then hit the submit button.

Now run the retriever component again, using the same command from above:

  maise> ant retriever -Danswers=maisetutorial-c01.answers.log -Dfield=keywords -Dquery=maisetutorial-c01

This time, the answers for one HIT (the one you answered) will be retrieved and
written to the answers log.  If you take a look at the log now, you will see
the same answers that you just provided when answering the HIT.

Now do a few more HITs from that same batch to make things a little more
interesting.  Once you've done that, execute the retriever command yet again:

  maise> ant retriever -Danswers=maisetutorial-c01.answers.log -Dfield=keywords -Dquery=maisetutorial-c01 > retriever.out

(Note that we are redirecting the output to retriever.out, since there's getting
to be too much of it for the screen.)

What the retriever does, before anything else, is to first process the
*existing* answers log and take note of the existing entries, in order to avoid
duplicating that entry when the retriever comes across it again.

When you use MAISE to perform an actual evaluation on MTurk, you should run the
retriever fairly regularly, perhaps once every day or two.  Don't wait too long
to run the retriever module, because workers will notice that you take a long
time to pay them and might therefore be hesitant to answer any more HITs from
your tasks.  Word of mouth might also harm you if you earn a reputation for
being lazy with paying workers, and that would make more people more hesitant to
do your tasks.

---------

Let's assume that all your HITs have been completed, and now you want to remove
your HITs from the MTurk system.  Or perhaps you think you have enough
annotations already, and want to remove your HITs because you don't want to pay
any more money.  In this case, you will use the "cleaner" component.  From the
maise/ path, execute the following command:

  maise> ant cleaner -DdelAssignable=true -DdelCompleted=true -Dfield=keywords -Dquery=maisetutorial-c01

The claner module is quite similar to the retriever, in that it finds all your
HITs that satsify a particular search criterion.  In this case, again, we get
HITs with "maisetutorial-c01" in the keywords field.  Among those HITs, the
cleaner will delete any assignable HITs (i.e. HITs that are active and have at
least one assignment unfulfilled) because the delAssigmable flag is set to true.
The cleaner will also delete any reviewable HITs (i.e. HITs that have had all
their assignments fulfilled) because the delReviewable flag is set to true.

In this case, since we set both flags to true, all the HITs that match the
search criterion will be deleted.

You can check for yourself that the HITs have been deleted by checking online
again:

  https://requestersandbox.mturk.com/mturk/manageHITs

...and seeing that no HITs are left.

Of course, even though the HITs are gone, you do have the answers provided by
workers, since you retrieved those answers and they are now saved in the answers
log file.

---------

Now that you have an answers log file, you are free to process it any way you
want.  You can write your own code to read the answers and make some sense out
of them.  Or, alternatively, you can use the analysis tool for this ranking task
that already comes with MAISE, called AnalyzeResults.  From the maise/ path,
execute the following command:

  maise> java -cp lib/ AnalyzeRNKResults answers=maisetutorial-c01.answers.log collection=maisetutorial-c01 serverInfo=maisetutorial/maisetutorial_output/maisetutorial_server_info.txt

Here is what the three command-line arguments mean:

  (*) answers: the answers file (created by the retriever module, remember?)
  (*) collection: the collection ID for data you are interested in analyzing
      (this is useful if you ever have a single answers file that has answers
      from multiple collections)
  (*) serverInfo: the file created by CreateServerInfo

There are three other arguments, which are optional:

  (*) turkInputLoc: the directory containing the MTurk upload files, created by
      the CreateBatches component.  (This is useful because sometimes the HIT
      interface fails to preserve certain data such as sentence indices and
      system ID's.  Luckily, we already have this info in the upload files, and
      we can recover.  Also luckily, this happens very rarely, usually when the
      worker has some kind of blocking plugins.)
  (*) filterList: a list of worker ID's whose data you want excluded from the analysis.
  (*) agreeAnalysis: set to 0 if you don't want annotator agreement analysis completed,
                     set to 1 if you want inter-annotator agreement analysis completed
                     set to 2 if you want inter- AND intra-annotator agreement analysis completed
      The default value is 1.  Note that setting 2 takes longer than setting 1,
      which takes longer than setting 0.

So if you use all six arguments, the command would be:

  maise> java -cp lib/ AnalyzeRNKResults answers=maisetutorial-c01.answers.log collection=maisetutorial-c01 serverInfo=maisetutorial/maisetutorial_output/maisetutorial_server_info.txt turkInputLoc=maisetutorial/maisetutorial_output/ filterList=maisetutorial/dummyFilterList.txt agreeAnalysis=1

In this case, the two commands above are equivalent.

---------

And what will you get?  The analysis tool will create two important files: a CSV
(i.e. comma-separated values) file that contains the labels provided by the
workers, and an HTML file that contains a summary of the analysis performed.

Each entry row in the CSV file corresponds to a single ranking set, and in
addition to the rank labels themselves, also contains the annotator's ID and
other relevant information about the sentences judged.

The HTML file (which you do have to view in a browser) is a bit more
interesting, as it includes an analysis of the data, and in partidular a ranking
of the systems based on how well they did.  The analyzer treats a rank set as an
implicit group of pairwise comparisons.  For instance, if the task involves
ranking outputs in sets of five, you obtain (5 choose 2) = 10 pairwise
comparisons.  A good system would win (or tie) a high percentage of those
comparisons, and a bad system would not.

You will also find a summary of each worker's annotations in the form of "worker
profiles".  Each row in this table corresponds to one worker, identified by
their Amazon worker ID, and includes certain measures that can help guide you
identify bad workers, who are either clicking randomly, or perhaps simply not
doing the task properly.

If you would like to see a more interesting output of the analysis tool, take
a look at the file maisetutorial/example.analysis.html .  The file was produced
after analyzing more than 85,000 actual rank labels in an internal evaluation
campaign of MT systems over 6 language pairs.  (The system names have been
anonymized.)  Hopefully it will give you an idea of what to expect when
evaluating your systems on MTurk.

---------

It is recommended that you run the analysis tool after every execution of the
retriever module.  This will enable you to detect bad workers before they do
too much work for you (using indicators like the RPR, etc).

But what should you do if you do find a bad worker completing your tasks?  You
can block that worker using MTurk's worker management page, which is here:

  https://requester.mturk.com/bulk/workers

If the worker you want to block has the ID AXXXXXXXXXXXXX, simply go to:

  https://requester.mturk.com/workers/AXXXXXXXXXXXXX

...and click on the block link on the worker's page.  Once a worker is blocked,
they will be unable to complete any more of your HITs.

Don't be too aggressive when blocking workers!  Make sure it's for a good
reason, because it's a big deal (if a worker gets a few blocks, their account
will be suspended).  If you block too many workers, you might gain a reputation
as a mean requester and workers will be hesitant to complete your task.
