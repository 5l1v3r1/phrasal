#!/usr/bin/env python
#
# Convert source side analysis to an R data frame. The source
# side analysis is generated by edu.stanford.nlp.ptm.SourceTextAnalysis.
#
#
import sys
import re
import codecs
import os
import csv
from os.path import basename,dirname
from collections import namedtuple
from csv_unicode import UnicodeReader,UnicodeWriter
from argparse import ArgumentParser

# Sentence-level data
SentenceData = namedtuple('SentenceData', 'src_id syn_complexity n_entity_tokens')

# Token-level data
TokenData = namedtuple('TokenData', 'src_id token_id token pos')

# Data frame definition
Row = namedtuple('Row', 'src_id src_len syn_complexity norm_syn_complexity n_entity_tokens norm_n_entity_tokens')


def get_annotated_src_doc(token_data_file):
    """ Converts token-level output of SourceTextAnalysis to lists of
    (token,tag) tuples.

    Args:
    Returns:
    Raises:
    """
    src_doc = []
    with open(token_data_file) as in_file:
        labeled_line = []
        last_src_id = 0
        for row in map(TokenData._make, UnicodeReader(in_file, delimiter='\t')):
            if last_src_id == int(row.src_id):
                labeled_line.append((row.token, row.pos))
            else:
                src_doc.append(labeled_line)
                labeled_line = []
            last_src_id = int(row.src_id)
        src_doc.append(labeled_line)
    return src_doc

def make_frame(token_data, sentence_data_file, output_file):
    """ Create the data frames from the raw input files.

    Args:
    Returns:
    Raises:
    """
    src_doc = get_annotated_src_doc(token_data)
    with open(sentence_data_file) as in_file:
        with open(output_file,'w') as out_file:
            csv_writer = csv.writer(out_file)
            for i,row in enumerate(map(SentenceData._make, csv.reader(in_file, delimiter='\t'))):
                src_id = int(row.src_id)
                n_tokens = len(src_doc[src_id])
                norm_complexity = float(row.syn_complexity) / float(n_tokens)
                norm_entity_tokens = float(row.n_entity_tokens) / float(n_tokens)
                outrow = Row(src_id=row.src_id,
                             src_len = str(n_tokens),
                             syn_complexity=row.syn_complexity,
                             norm_syn_complexity=str(norm_complexity),
                             n_entity_tokens=row.n_entity_tokens,
                             norm_n_entity_tokens=str(norm_entity_tokens))
                if i == 0:
                    csv_writer.writerow(list(outrow._fields))
                csv_writer.writerow([x for x in outrow._asdict().itervalues()])

                
def main():
    desc='Make source characteristics data frame'
    parser=ArgumentParser(description=desc)
    parser.add_argument('token_data',
                        help='Token-level data from SourceTextAnalysis.')
    parser.add_argument('sentence_data',
                        help='Sentence level data from SourceTextAnalysis.')
    parser.add_argument('output_file',
                        help='Output file name.')
    args = parser.parse_args()

    make_frame(args.token_data, args.sentence_data, args.output_file)

if __name__ == '__main__':
    main()
